{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 0. Basic\n\n### Display the some first `image_file_names` & `file_name.csv`"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\n## print out the names of the first 5 image_files (total = 10 images for train_imgaes & train_label_masks) with the train, test, submission.csv files & 5 file.hdf5\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:3]:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/pytable-hdf/img_dtbase_3750.h5\n/kaggle/input/pytable-hdf/img_dtbase_10250.h5\n/kaggle/input/pytable-hdf/img_dtbase_5000.h5\n/kaggle/input/h5-files/full_data_coordinate.h5\n/kaggle/input/prostate-cancer-grade-assessment/train.csv\n/kaggle/input/prostate-cancer-grade-assessment/test.csv\n/kaggle/input/prostate-cancer-grade-assessment/sample_submission.csv\n/kaggle/input/prostate-cancer-grade-assessment/train_images/eac2bfcf6b1dd7733ab9e2f31452e5bd.tiff\n/kaggle/input/prostate-cancer-grade-assessment/train_images/6ccb8529cc90ff30751eff7a1d055b6e.tiff\n/kaggle/input/prostate-cancer-grade-assessment/train_images/5f204e4117e261c6bfb06626e5bb2410.tiff\n/kaggle/input/prostate-cancer-grade-assessment/train_label_masks/3e16a4a0816d8380163af60dfc884195_mask.tiff\n/kaggle/input/prostate-cancer-grade-assessment/train_label_masks/3639415568d06af82c3cfcfeeae5d9b6_mask.tiff\n/kaggle/input/prostate-cancer-grade-assessment/train_label_masks/466a84e8d13813e7aabfe440f437d990_mask.tiff\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 1. Basic libraries\n\n### Loading & viewing dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\n\nimport openslide\nimport skimage.io\nimport random\nimport seaborn as sns\nimport cv2\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport PIL\nfrom IPython.display import Image, display\n\nimport plotly.graph_objs as go\n\nBASE_PATH = '../input/prostate-cancer-grade-assessment'\n\ndata_dir = f'{BASE_PATH}/train_images'\nmask_dir = f'{BASE_PATH}/train_label_masks'\n\ntrain = pd.read_csv(f'{BASE_PATH}/train.csv').set_index('image_id')\ntest = pd.read_csv(f'{BASE_PATH}/test.csv')\n\ntrain_labels = pd.read_csv('/kaggle/input/prostate-cancer-grade-assessment/train.csv').set_index('image_id')\n\nsubmission = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Create the `mutual_id_image` \n\nIn the previous sesion, we have show that it concided to the `mask_dir`\n\nBeside that, at each time we enter to the kernel, the arrangement of the img_id may be `changed`, so to fixed this for the partner, I will sort all of them before going on the tasks"},{"metadata":{"trusted":true},"cell_type":"code","source":"id_dir = [name.replace('_mask.tiff', '') for name in os.listdir(mask_dir)]\nid_dir.sort()\nprint(id_dir[:5])\n\nid_dir.remove('3790f55cad63053e956fb73027179707')\nimg_id_mutual_name = id_dir\nimg_id_mutual_name[:5], len(img_id_mutual_name)","execution_count":3,"outputs":[{"output_type":"stream","text":"['0005f7aaab2800f6170c399693a96917', '000920ad0b612851f8e01bcc880d9b3d', '0018ae58b01bdadc8e347995b69f99aa', '001c62abd11fa4b57bf7a6c603a11bb9', '001d865e65ef5d2579c190a0e0350d8f']\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"(['0005f7aaab2800f6170c399693a96917',\n  '000920ad0b612851f8e01bcc880d9b3d',\n  '0018ae58b01bdadc8e347995b69f99aa',\n  '001c62abd11fa4b57bf7a6c603a11bb9',\n  '001d865e65ef5d2579c190a0e0350d8f'],\n 10515)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import deepdish as dd\n\ndf = dd.io.load('/kaggle/input/h5-files/full_data_coordinate.h5')\nlen(df)//36, len(df[0]), df[0]","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(9790, 5, ['0005f7aaab2800f6170c399693a96917', 13312, 13824, 7168, 7680])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data_and_mask(ID, coordinates, level = 2):\n    \"\"\"\n    Args:\n        ID\n        coordinates\n        level {0, 1, 2}: \n    return : 3D arrays of data & mask image\n    \"\"\"\n    data_img = skimage.io.MultiImage(os.path.join(data_dir, f'{ID}.tiff'))[level]\n    mask_img = skimage.io.MultiImage(os.path.join(mask_dir, f'{ID}_mask.tiff'))[level]\n    coordinates = [coordinate // 2**(2*level) for coordinate in coordinates]\n    data_tile = data_img[coordinates[0]: coordinates[1], coordinates[2]: coordinates[3], :]\n    mask_tile = mask_img[coordinates[0]: coordinates[1], coordinates[2]: coordinates[3], :]\n    data_tile = cv2.resize(data_tile, (512, 512))\n    mask_tile = cv2.resize(mask_tile, (512, 512))\n    del data_img, mask_img\n    \n    # Load and return small image\n    return data_tile, mask_tile","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\n\nclass PANDADataset(Dataset):\n    def __init__(self, df, level = 2, transform=None):\n        self.df = df\n        self.level = level\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, index, level = 2):\n        ID = self.df[index][0]\n        coordinate = self.df[index][1: ]\n        image, mask = load_data_and_mask(ID, coordinate, level)\n        \n        return torch.tensor(image).permute(2, 0, 1), torch.tensor(mask).permute(2, 0, 1)[0]\n    \ncls = PANDADataset(df[:200000], 2)\n%time cls[0][0].size(), cls[0][1].size(), len(cls)","execution_count":6,"outputs":[{"output_type":"stream","text":"CPU times: user 98.8 ms, sys: 14.7 ms, total: 114 ms\nWall time: 178 ms\n","name":"stdout"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(torch.Size([3, 512, 512]), torch.Size([512, 512]), 200000)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataLoader = DataLoader(cls, batch_size=8, shuffle=True, num_workers=8)\nprint(len(dataLoader))\ndel df, cls","execution_count":7,"outputs":[{"output_type":"stream","text":"25000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adapted from https://discuss.pytorch.org/t/unet-implementation/426\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,\n                 batch_norm=False, up_mode='upconv'):\n        \"\"\"\n        Implementation of\n        U-Net: Convolutional Networks for Biomedical Image Segmentation\n        (Ronneberger et al., 2015)\n        https://arxiv.org/abs/1505.04597\n        Using the default arguments will yield the exact version used\n        in the original paper\n        Args:\n            in_channels (int): number of input channels\n            n_classes (int): number of output channels\n            depth (int): depth of the network\n            wf (int): number of filters in the first layer is 2**wf\n            padding (bool): if True, apply padding such that the input shape\n                            is the same as the output.\n                            This may introduce artifacts\n            batch_norm (bool): Use BatchNorm after layers with an\n                               activation function\n            up_mode (str): one of 'upconv' or 'upsample'.\n                           'upconv' will use transposed convolutions for\n                           learned upsampling.\n                           'upsample' will use bilinear upsampling.\n        \"\"\"\n        super(UNet, self).__init__()\n        assert up_mode in ('upconv', 'upsample')\n        self.padding = padding\n        self.depth = depth\n        prev_channels = in_channels\n        self.down_path = nn.ModuleList()\n        for i in range(depth):\n            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n                                                padding, batch_norm))\n            prev_channels = 2**(wf+i)\n\n        self.up_path = nn.ModuleList()\n        for i in reversed(range(depth - 1)):\n            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n                                            padding, batch_norm))\n            prev_channels = 2**(wf+i)\n\n        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n\n    def forward(self, x):\n        blocks = []\n        for i, down in enumerate(self.down_path):\n            x = down(x)\n            if i != len(self.down_path)-1:\n                blocks.append(x)\n                x = F.avg_pool2d(x, 2)\n\n        for i, up in enumerate(self.up_path):\n            x = up(x, blocks[-i-1])\n\n        return self.last(x)\n\n\nclass UNetConvBlock(nn.Module):\n    def __init__(self, in_size, out_size, padding, batch_norm):\n        super(UNetConvBlock, self).__init__()\n        block = []\n\n        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n                               padding=int(padding)))\n        block.append(nn.ReLU())\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n\n        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n                               padding=int(padding)))\n        block.append(nn.ReLU())\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n\n        self.block = nn.Sequential(*block)\n\n    def forward(self, x):\n        out = self.block(x)\n        return out\n\n\nclass UNetUpBlock(nn.Module):\n    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n        super(UNetUpBlock, self).__init__()\n        if up_mode == 'upconv':\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n                                         stride=2)\n        elif up_mode == 'upsample':\n            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n\n        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n\n    def center_crop(self, layer, target_size):\n        _, _, layer_height, layer_width = layer.size()\n        diff_y = (layer_height - target_size[0]) // 2\n        diff_x = (layer_width - target_size[1]) // 2\n        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n\n    def forward(self, x, bridge):\n        up = self.up(x)\n        crop1 = self.center_crop(bridge, up.shape[2:])\n        out = torch.cat([up, crop1], 1)\n        out = self.conv_block(out)\n\n        return out","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- unet params\n#these parameters get fed directly into the UNET class, and more description of them can be discovered there\n\nn_classes = 6    #number of classes in the data mask that we'll aim to predict\n\n\nin_channels= 3  #input channel of the data, RGB = 3\npadding= True   #should levels be padded\ndepth= 5       #depth of the network \nwf= 2           #wf (int): number of filters in the first layer is 2**wf, was 6\nup_mode= 'upconv' #should we simply upsample the mask, or should we try and learn an interpolation \nbatch_norm = True #should we use batch normalization between the layers\n\n# --- training params\nbatch_size = 8\npatch_size = 512\nnum_epochs = 3\nedge_weight = 1.1 #edges tend to be the most poorly segmented given how little area they occupy in the training set, this paramter boosts their values along the lines of the original UNET paper\nphases = [\"train\",\"val\"] #how many phases did we create databases for?\nvalidation_phases= [\"val\"] #when should we do valiation? note that validation is time consuming, so as opposed to doing for both training and validation, we do it only for vlaidation at the end of the epoch","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpuid=0\n#specify if we should use a GPU (cuda) or only the CPU\nif(torch.cuda.is_available()):\n    print(torch.cuda.get_device_properties(gpuid))\n    torch.cuda.set_device(gpuid)\n    device = torch.device(f'cuda:{gpuid}')\nelse:\n    device = torch.device(f'cpu')","execution_count":10,"outputs":[{"output_type":"stream","text":"_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = UNet(n_classes=n_classes, in_channels=in_channels, \n             padding=padding,depth=depth,wf=wf, \n             up_mode=up_mode, batch_norm=batch_norm).to(device)\nprint(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")\n\noptim = torch.optim.Adam(model.parameters()) #adam is going to be the most robust\ncriterion = nn.CrossEntropyLoss(reduce=False)","execution_count":11,"outputs":[{"output_type":"stream","text":"total params: \t122486\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n  warnings.warn(warning.format(ret))\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nprint('======================================== Training started ========================================')\nfor epoch in range(num_epochs):\n    print('==================================================================================================')\n    #model.train()  # Set model to training mode\n    running_loss = 0.0\n    t0 = time.time()\n    for i, data in enumerate(dataLoader, 0):\n        inputs, labels = data\n        inputs = inputs.to(device,dtype=torch.float) \n        labels = labels.type('torch.LongTensor').to(device)\n        ##with torch.set_grad_enabled(True):\n        # zero the parameter gradients\n        optim.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.sum().backward()\n        optim.step()\n        # print statistics\n        \n        running_loss += loss.mean()\n        \n        if i % 400 == 399:    # print every 400 mini-batches\n            t1 = time.time()\n            print('Epoch %02d, upto %05d mini-batches; after %02d (minutes) and %02d (seconds);  train_loss = %.3f'%\n                  (epoch + 1, i + 1, (t1 - t0) // 60, (t1 - t0)%60, running_loss / 400))\n            running_loss = 0.0\nprint('==================================================================================================')\nprint('======================================== Finished Training =======================================')","execution_count":12,"outputs":[{"output_type":"stream","text":"======================================== Training started ========================================\n==================================================================================================\nEpoch 01, upto 00400 mini-batches; after 01 (minutes) and 37 (seconds);  train_loss = 1.601\nEpoch 01, upto 00800 mini-batches; after 03 (minutes) and 09 (seconds);  train_loss = 1.089\nEpoch 01, upto 01200 mini-batches; after 04 (minutes) and 40 (seconds);  train_loss = 1.035\nEpoch 01, upto 01600 mini-batches; after 06 (minutes) and 12 (seconds);  train_loss = 1.018\nEpoch 01, upto 02000 mini-batches; after 07 (minutes) and 44 (seconds);  train_loss = 1.008\nEpoch 01, upto 02400 mini-batches; after 09 (minutes) and 16 (seconds);  train_loss = 1.010\nEpoch 01, upto 02800 mini-batches; after 10 (minutes) and 49 (seconds);  train_loss = 0.965\nEpoch 01, upto 03200 mini-batches; after 12 (minutes) and 21 (seconds);  train_loss = 0.967\nEpoch 01, upto 03600 mini-batches; after 13 (minutes) and 54 (seconds);  train_loss = 0.962\nEpoch 01, upto 04000 mini-batches; after 15 (minutes) and 26 (seconds);  train_loss = 0.921\nEpoch 01, upto 04400 mini-batches; after 16 (minutes) and 59 (seconds);  train_loss = 0.940\nEpoch 01, upto 04800 mini-batches; after 18 (minutes) and 31 (seconds);  train_loss = 0.917\nEpoch 01, upto 05200 mini-batches; after 20 (minutes) and 04 (seconds);  train_loss = 0.910\nEpoch 01, upto 05600 mini-batches; after 21 (minutes) and 37 (seconds);  train_loss = 0.913\nEpoch 01, upto 06000 mini-batches; after 23 (minutes) and 11 (seconds);  train_loss = 0.916\nEpoch 01, upto 06400 mini-batches; after 24 (minutes) and 43 (seconds);  train_loss = 0.910\nEpoch 01, upto 06800 mini-batches; after 26 (minutes) and 17 (seconds);  train_loss = 0.908\nEpoch 01, upto 07200 mini-batches; after 27 (minutes) and 48 (seconds);  train_loss = 0.906\nEpoch 01, upto 07600 mini-batches; after 29 (minutes) and 20 (seconds);  train_loss = 0.890\nEpoch 01, upto 08000 mini-batches; after 30 (minutes) and 53 (seconds);  train_loss = 0.921\nEpoch 01, upto 08400 mini-batches; after 32 (minutes) and 25 (seconds);  train_loss = 0.895\nEpoch 01, upto 08800 mini-batches; after 33 (minutes) and 59 (seconds);  train_loss = 0.892\nEpoch 01, upto 09200 mini-batches; after 35 (minutes) and 30 (seconds);  train_loss = 0.879\nEpoch 01, upto 09600 mini-batches; after 37 (minutes) and 04 (seconds);  train_loss = 0.889\nEpoch 01, upto 10000 mini-batches; after 38 (minutes) and 36 (seconds);  train_loss = 0.898\nEpoch 01, upto 10400 mini-batches; after 40 (minutes) and 08 (seconds);  train_loss = 0.895\nEpoch 01, upto 10800 mini-batches; after 41 (minutes) and 42 (seconds);  train_loss = 0.875\nEpoch 01, upto 11200 mini-batches; after 43 (minutes) and 15 (seconds);  train_loss = 0.895\nEpoch 01, upto 11600 mini-batches; after 44 (minutes) and 47 (seconds);  train_loss = 0.889\nEpoch 01, upto 12000 mini-batches; after 46 (minutes) and 18 (seconds);  train_loss = 0.881\nEpoch 01, upto 12400 mini-batches; after 47 (minutes) and 52 (seconds);  train_loss = 0.894\nEpoch 01, upto 12800 mini-batches; after 49 (minutes) and 23 (seconds);  train_loss = 0.875\nEpoch 01, upto 13200 mini-batches; after 50 (minutes) and 55 (seconds);  train_loss = 0.884\nEpoch 01, upto 13600 mini-batches; after 52 (minutes) and 28 (seconds);  train_loss = 0.879\nEpoch 01, upto 14000 mini-batches; after 54 (minutes) and 02 (seconds);  train_loss = 0.886\nEpoch 01, upto 14400 mini-batches; after 55 (minutes) and 34 (seconds);  train_loss = 0.886\nEpoch 01, upto 14800 mini-batches; after 57 (minutes) and 08 (seconds);  train_loss = 0.869\nEpoch 01, upto 15200 mini-batches; after 58 (minutes) and 40 (seconds);  train_loss = 0.871\nEpoch 01, upto 15600 mini-batches; after 60 (minutes) and 13 (seconds);  train_loss = 0.856\nEpoch 01, upto 16000 mini-batches; after 61 (minutes) and 43 (seconds);  train_loss = 0.863\nEpoch 01, upto 16400 mini-batches; after 63 (minutes) and 18 (seconds);  train_loss = 0.874\nEpoch 01, upto 16800 mini-batches; after 64 (minutes) and 51 (seconds);  train_loss = 0.852\nEpoch 01, upto 17200 mini-batches; after 66 (minutes) and 24 (seconds);  train_loss = 0.861\nEpoch 01, upto 17600 mini-batches; after 67 (minutes) and 58 (seconds);  train_loss = 0.877\nEpoch 01, upto 18000 mini-batches; after 69 (minutes) and 31 (seconds);  train_loss = 0.861\nEpoch 01, upto 18400 mini-batches; after 71 (minutes) and 05 (seconds);  train_loss = 0.867\nEpoch 01, upto 18800 mini-batches; after 72 (minutes) and 38 (seconds);  train_loss = 0.873\nEpoch 01, upto 19200 mini-batches; after 74 (minutes) and 11 (seconds);  train_loss = 0.855\nEpoch 01, upto 19600 mini-batches; after 75 (minutes) and 45 (seconds);  train_loss = 0.878\nEpoch 01, upto 20000 mini-batches; after 77 (minutes) and 19 (seconds);  train_loss = 0.843\nEpoch 01, upto 20400 mini-batches; after 78 (minutes) and 53 (seconds);  train_loss = 0.843\nEpoch 01, upto 20800 mini-batches; after 80 (minutes) and 23 (seconds);  train_loss = 0.847\nEpoch 01, upto 21200 mini-batches; after 81 (minutes) and 58 (seconds);  train_loss = 0.863\nEpoch 01, upto 21600 mini-batches; after 83 (minutes) and 31 (seconds);  train_loss = 0.836\nEpoch 01, upto 22000 mini-batches; after 85 (minutes) and 04 (seconds);  train_loss = 0.855\nEpoch 01, upto 22400 mini-batches; after 86 (minutes) and 36 (seconds);  train_loss = 0.846\nEpoch 01, upto 22800 mini-batches; after 88 (minutes) and 10 (seconds);  train_loss = 0.840\nEpoch 01, upto 23200 mini-batches; after 89 (minutes) and 43 (seconds);  train_loss = 0.857\nEpoch 01, upto 23600 mini-batches; after 91 (minutes) and 16 (seconds);  train_loss = 0.834\nEpoch 01, upto 24000 mini-batches; after 92 (minutes) and 50 (seconds);  train_loss = 0.843\nEpoch 01, upto 24400 mini-batches; after 94 (minutes) and 23 (seconds);  train_loss = 0.837\nEpoch 01, upto 24800 mini-batches; after 95 (minutes) and 58 (seconds);  train_loss = 0.860\n==================================================================================================\nEpoch 02, upto 00400 mini-batches; after 01 (minutes) and 34 (seconds);  train_loss = 0.860\nEpoch 02, upto 00800 mini-batches; after 03 (minutes) and 08 (seconds);  train_loss = 0.838\nEpoch 02, upto 01200 mini-batches; after 04 (minutes) and 41 (seconds);  train_loss = 0.834\nEpoch 02, upto 01600 mini-batches; after 06 (minutes) and 15 (seconds);  train_loss = 0.853\nEpoch 02, upto 02000 mini-batches; after 07 (minutes) and 47 (seconds);  train_loss = 0.832\nEpoch 02, upto 02400 mini-batches; after 09 (minutes) and 22 (seconds);  train_loss = 0.829\nEpoch 02, upto 02800 mini-batches; after 10 (minutes) and 54 (seconds);  train_loss = 0.832\nEpoch 02, upto 03200 mini-batches; after 12 (minutes) and 29 (seconds);  train_loss = 0.846\nEpoch 02, upto 03600 mini-batches; after 14 (minutes) and 03 (seconds);  train_loss = 0.846\nEpoch 02, upto 04000 mini-batches; after 15 (minutes) and 36 (seconds);  train_loss = 0.821\nEpoch 02, upto 04400 mini-batches; after 17 (minutes) and 10 (seconds);  train_loss = 0.836\nEpoch 02, upto 04800 mini-batches; after 18 (minutes) and 43 (seconds);  train_loss = 0.857\nEpoch 02, upto 05200 mini-batches; after 20 (minutes) and 18 (seconds);  train_loss = 0.838\nEpoch 02, upto 05600 mini-batches; after 21 (minutes) and 51 (seconds);  train_loss = 0.843\nEpoch 02, upto 06000 mini-batches; after 23 (minutes) and 25 (seconds);  train_loss = 0.846\nEpoch 02, upto 06400 mini-batches; after 24 (minutes) and 58 (seconds);  train_loss = 0.832\nEpoch 02, upto 06800 mini-batches; after 26 (minutes) and 33 (seconds);  train_loss = 0.851\nEpoch 02, upto 07200 mini-batches; after 28 (minutes) and 07 (seconds);  train_loss = 0.842\nEpoch 02, upto 07600 mini-batches; after 29 (minutes) and 40 (seconds);  train_loss = 0.856\nEpoch 02, upto 08000 mini-batches; after 31 (minutes) and 14 (seconds);  train_loss = 0.836\nEpoch 02, upto 08400 mini-batches; after 32 (minutes) and 48 (seconds);  train_loss = 0.811\nEpoch 02, upto 08800 mini-batches; after 34 (minutes) and 22 (seconds);  train_loss = 0.828\nEpoch 02, upto 09200 mini-batches; after 35 (minutes) and 56 (seconds);  train_loss = 0.824\nEpoch 02, upto 09600 mini-batches; after 37 (minutes) and 29 (seconds);  train_loss = 0.827\n","name":"stdout"},{"output_type":"stream","text":"Epoch 02, upto 10000 mini-batches; after 39 (minutes) and 03 (seconds);  train_loss = 0.844\nEpoch 02, upto 10400 mini-batches; after 40 (minutes) and 37 (seconds);  train_loss = 0.838\nEpoch 02, upto 10800 mini-batches; after 42 (minutes) and 10 (seconds);  train_loss = 0.809\nEpoch 02, upto 11200 mini-batches; after 43 (minutes) and 45 (seconds);  train_loss = 0.832\nEpoch 02, upto 11600 mini-batches; after 45 (minutes) and 18 (seconds);  train_loss = 0.825\nEpoch 02, upto 12000 mini-batches; after 46 (minutes) and 52 (seconds);  train_loss = 0.814\nEpoch 02, upto 12400 mini-batches; after 48 (minutes) and 26 (seconds);  train_loss = 0.806\nEpoch 02, upto 12800 mini-batches; after 49 (minutes) and 59 (seconds);  train_loss = 0.864\nEpoch 02, upto 13200 mini-batches; after 51 (minutes) and 32 (seconds);  train_loss = 0.837\nEpoch 02, upto 13600 mini-batches; after 53 (minutes) and 06 (seconds);  train_loss = 0.830\nEpoch 02, upto 14000 mini-batches; after 54 (minutes) and 38 (seconds);  train_loss = 0.836\nEpoch 02, upto 14400 mini-batches; after 56 (minutes) and 13 (seconds);  train_loss = 0.820\nEpoch 02, upto 14800 mini-batches; after 57 (minutes) and 45 (seconds);  train_loss = 0.817\nEpoch 02, upto 15200 mini-batches; after 59 (minutes) and 19 (seconds);  train_loss = 0.822\nEpoch 02, upto 15600 mini-batches; after 60 (minutes) and 50 (seconds);  train_loss = 0.833\nEpoch 02, upto 16000 mini-batches; after 62 (minutes) and 23 (seconds);  train_loss = 0.817\nEpoch 02, upto 16400 mini-batches; after 63 (minutes) and 56 (seconds);  train_loss = 0.821\nEpoch 02, upto 16800 mini-batches; after 65 (minutes) and 30 (seconds);  train_loss = 0.830\nEpoch 02, upto 17200 mini-batches; after 67 (minutes) and 01 (seconds);  train_loss = 0.834\nEpoch 02, upto 17600 mini-batches; after 68 (minutes) and 34 (seconds);  train_loss = 0.814\nEpoch 02, upto 18000 mini-batches; after 70 (minutes) and 07 (seconds);  train_loss = 0.824\nEpoch 02, upto 18400 mini-batches; after 71 (minutes) and 40 (seconds);  train_loss = 0.822\nEpoch 02, upto 18800 mini-batches; after 73 (minutes) and 12 (seconds);  train_loss = 0.812\nEpoch 02, upto 19200 mini-batches; after 74 (minutes) and 43 (seconds);  train_loss = 0.824\nEpoch 02, upto 19600 mini-batches; after 76 (minutes) and 15 (seconds);  train_loss = 0.813\nEpoch 02, upto 20000 mini-batches; after 77 (minutes) and 47 (seconds);  train_loss = 0.835\nEpoch 02, upto 20400 mini-batches; after 79 (minutes) and 20 (seconds);  train_loss = 0.838\nEpoch 02, upto 20800 mini-batches; after 80 (minutes) and 51 (seconds);  train_loss = 0.822\nEpoch 02, upto 21200 mini-batches; after 82 (minutes) and 24 (seconds);  train_loss = 0.780\nEpoch 02, upto 21600 mini-batches; after 83 (minutes) and 57 (seconds);  train_loss = 0.811\nEpoch 02, upto 22000 mini-batches; after 85 (minutes) and 32 (seconds);  train_loss = 0.798\nEpoch 02, upto 22400 mini-batches; after 87 (minutes) and 06 (seconds);  train_loss = 0.807\nEpoch 02, upto 22800 mini-batches; after 88 (minutes) and 40 (seconds);  train_loss = 0.826\nEpoch 02, upto 23200 mini-batches; after 90 (minutes) and 15 (seconds);  train_loss = 0.814\nEpoch 02, upto 23600 mini-batches; after 91 (minutes) and 48 (seconds);  train_loss = 0.799\nEpoch 02, upto 24000 mini-batches; after 93 (minutes) and 23 (seconds);  train_loss = 0.817\nEpoch 02, upto 24400 mini-batches; after 94 (minutes) and 56 (seconds);  train_loss = 0.811\nEpoch 02, upto 24800 mini-batches; after 96 (minutes) and 31 (seconds);  train_loss = 0.807\n==================================================================================================\nEpoch 03, upto 00400 mini-batches; after 01 (minutes) and 36 (seconds);  train_loss = 0.837\nEpoch 03, upto 00800 mini-batches; after 03 (minutes) and 10 (seconds);  train_loss = 0.807\nEpoch 03, upto 01200 mini-batches; after 04 (minutes) and 45 (seconds);  train_loss = 0.814\nEpoch 03, upto 01600 mini-batches; after 06 (minutes) and 19 (seconds);  train_loss = 0.797\nEpoch 03, upto 02000 mini-batches; after 07 (minutes) and 58 (seconds);  train_loss = 0.816\nEpoch 03, upto 02400 mini-batches; after 09 (minutes) and 31 (seconds);  train_loss = 0.807\nEpoch 03, upto 02800 mini-batches; after 11 (minutes) and 05 (seconds);  train_loss = 0.826\nEpoch 03, upto 03200 mini-batches; after 12 (minutes) and 37 (seconds);  train_loss = 0.809\nEpoch 03, upto 03600 mini-batches; after 14 (minutes) and 12 (seconds);  train_loss = 0.813\nEpoch 03, upto 04000 mini-batches; after 15 (minutes) and 47 (seconds);  train_loss = 0.829\nEpoch 03, upto 04400 mini-batches; after 17 (minutes) and 21 (seconds);  train_loss = 0.814\nEpoch 03, upto 04800 mini-batches; after 18 (minutes) and 56 (seconds);  train_loss = 0.803\nEpoch 03, upto 05200 mini-batches; after 20 (minutes) and 29 (seconds);  train_loss = 0.815\nEpoch 03, upto 05600 mini-batches; after 22 (minutes) and 04 (seconds);  train_loss = 0.794\nEpoch 03, upto 06000 mini-batches; after 23 (minutes) and 39 (seconds);  train_loss = 0.822\nEpoch 03, upto 06400 mini-batches; after 25 (minutes) and 13 (seconds);  train_loss = 0.827\nEpoch 03, upto 06800 mini-batches; after 26 (minutes) and 47 (seconds);  train_loss = 0.805\nEpoch 03, upto 07200 mini-batches; after 28 (minutes) and 22 (seconds);  train_loss = 0.812\nEpoch 03, upto 07600 mini-batches; after 29 (minutes) and 59 (seconds);  train_loss = 0.817\nEpoch 03, upto 08000 mini-batches; after 31 (minutes) and 33 (seconds);  train_loss = 0.813\nEpoch 03, upto 08400 mini-batches; after 33 (minutes) and 06 (seconds);  train_loss = 0.797\nEpoch 03, upto 08800 mini-batches; after 34 (minutes) and 39 (seconds);  train_loss = 0.806\nEpoch 03, upto 09200 mini-batches; after 36 (minutes) and 14 (seconds);  train_loss = 0.801\nEpoch 03, upto 09600 mini-batches; after 37 (minutes) and 49 (seconds);  train_loss = 0.828\nEpoch 03, upto 10000 mini-batches; after 39 (minutes) and 24 (seconds);  train_loss = 0.823\nEpoch 03, upto 10400 mini-batches; after 40 (minutes) and 58 (seconds);  train_loss = 0.816\nEpoch 03, upto 10800 mini-batches; after 42 (minutes) and 31 (seconds);  train_loss = 0.794\nEpoch 03, upto 11200 mini-batches; after 44 (minutes) and 06 (seconds);  train_loss = 0.814\nEpoch 03, upto 11600 mini-batches; after 45 (minutes) and 41 (seconds);  train_loss = 0.798\nEpoch 03, upto 12000 mini-batches; after 47 (minutes) and 15 (seconds);  train_loss = 0.802\nEpoch 03, upto 12400 mini-batches; after 48 (minutes) and 50 (seconds);  train_loss = 0.792\nEpoch 03, upto 12800 mini-batches; after 50 (minutes) and 26 (seconds);  train_loss = 0.807\nEpoch 03, upto 13200 mini-batches; after 52 (minutes) and 01 (seconds);  train_loss = 0.803\nEpoch 03, upto 13600 mini-batches; after 53 (minutes) and 34 (seconds);  train_loss = 0.805\nEpoch 03, upto 14000 mini-batches; after 55 (minutes) and 09 (seconds);  train_loss = 0.799\nEpoch 03, upto 14400 mini-batches; after 56 (minutes) and 48 (seconds);  train_loss = 0.811\nEpoch 03, upto 14800 mini-batches; after 58 (minutes) and 24 (seconds);  train_loss = 0.814\nEpoch 03, upto 15200 mini-batches; after 60 (minutes) and 00 (seconds);  train_loss = 0.812\nEpoch 03, upto 15600 mini-batches; after 61 (minutes) and 33 (seconds);  train_loss = 0.802\nEpoch 03, upto 16000 mini-batches; after 63 (minutes) and 07 (seconds);  train_loss = 0.802\nEpoch 03, upto 16400 mini-batches; after 64 (minutes) and 40 (seconds);  train_loss = 0.795\nEpoch 03, upto 16800 mini-batches; after 66 (minutes) and 15 (seconds);  train_loss = 0.799\nEpoch 03, upto 17200 mini-batches; after 67 (minutes) and 50 (seconds);  train_loss = 0.803\nEpoch 03, upto 17600 mini-batches; after 69 (minutes) and 23 (seconds);  train_loss = 0.819\nEpoch 03, upto 18000 mini-batches; after 70 (minutes) and 59 (seconds);  train_loss = 0.784\nEpoch 03, upto 18400 mini-batches; after 72 (minutes) and 32 (seconds);  train_loss = 0.805\nEpoch 03, upto 18800 mini-batches; after 74 (minutes) and 07 (seconds);  train_loss = 0.805\nEpoch 03, upto 19200 mini-batches; after 75 (minutes) and 39 (seconds);  train_loss = 0.803\nEpoch 03, upto 19600 mini-batches; after 77 (minutes) and 13 (seconds);  train_loss = 0.798\nEpoch 03, upto 20000 mini-batches; after 78 (minutes) and 47 (seconds);  train_loss = 0.802\n","name":"stdout"},{"output_type":"stream","text":"Epoch 03, upto 20400 mini-batches; after 80 (minutes) and 20 (seconds);  train_loss = 0.795\nEpoch 03, upto 20800 mini-batches; after 81 (minutes) and 54 (seconds);  train_loss = 0.804\nEpoch 03, upto 21200 mini-batches; after 83 (minutes) and 28 (seconds);  train_loss = 0.800\nEpoch 03, upto 21600 mini-batches; after 85 (minutes) and 01 (seconds);  train_loss = 0.809\nEpoch 03, upto 22000 mini-batches; after 86 (minutes) and 36 (seconds);  train_loss = 0.793\nEpoch 03, upto 22400 mini-batches; after 88 (minutes) and 09 (seconds);  train_loss = 0.796\nEpoch 03, upto 22800 mini-batches; after 89 (minutes) and 43 (seconds);  train_loss = 0.804\nEpoch 03, upto 23200 mini-batches; after 91 (minutes) and 18 (seconds);  train_loss = 0.789\nEpoch 03, upto 23600 mini-batches; after 92 (minutes) and 54 (seconds);  train_loss = 0.795\nEpoch 03, upto 24000 mini-batches; after 94 (minutes) and 28 (seconds);  train_loss = 0.788\nEpoch 03, upto 24400 mini-batches; after 96 (minutes) and 03 (seconds);  train_loss = 0.807\nEpoch 03, upto 24800 mini-batches; after 97 (minutes) and 38 (seconds);  train_loss = 0.808\n==================================================================================================\n======================================== Finished Training =======================================\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dataLoader)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"torch.utils.data.dataloader.DataLoader"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### 60000 tiles; bacth_size = 6; count on per 400 mini-batches ==> from 11.6 to 12.2 Gb; GPU maximum 49%; CPU maximum 191%\n#### 72000 tiles; bacth_size = 8; count on per 1000 mini-batches => from 11.7 to 13 Gb; GPU maximum 72%; CPU maximum 198%\n#### 200000 tiles; bacth_size = 8; count on per 400 mini-batches => from 11.7 to 12.8 Gb; GPU maximum 70%; CPU maximum 188%"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}