{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Basic\n",
    "\n",
    "### Display the some first `image_file_names` & `file_name.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pytable-hdf/img_dtbase_9250.h5\n",
      "/kaggle/input/pytable-hdf/img_dtbase_8500.h5\n",
      "/kaggle/input/h5-files/full_data_coordinate.h5\n",
      "/kaggle/input/prostate-cancer-grade-assessment/sample_submission.csv\n",
      "/kaggle/input/prostate-cancer-grade-assessment/test.csv\n",
      "/kaggle/input/prostate-cancer-grade-assessment/train_images/a05ded7fe107cfdfcfc8f644fb2d7313.tiff\n",
      "/kaggle/input/prostate-cancer-grade-assessment/train_images/72e64850d127c65815492af84473a26e.tiff\n",
      "/kaggle/input/prostate-cancer-grade-assessment/train_label_masks/aa7000449538fe951e1c0dadc7ce9b44_mask.tiff\n",
      "/kaggle/input/prostate-cancer-grade-assessment/train_label_masks/d63c41a88f43f4f46b49fe4d63c9d117_mask.tiff\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "\n",
    "## print out the names of the first 5 image_files (total = 10 images for train_imgaes & train_label_masks) with the train, test, submission.csv files & 5 file.hdf5\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames[:2]:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic libraries\n",
    "\n",
    "### Loading & viewing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openslide\n",
    "import skimage.io\n",
    "import random\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "BASE_PATH = '../input/prostate-cancer-grade-assessment'\n",
    "\n",
    "data_dir = f'{BASE_PATH}/train_images'\n",
    "mask_dir = f'{BASE_PATH}/train_label_masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the `mutual_id_image` \n",
    "\n",
    "In the previous sesion, we have show that it concided to the `mask_dir`\n",
    "\n",
    "Beside that, at each time we enter to the kernel, the arrangement of the img_id may be `changed`, so to fixed this for the partner, I will sort all of them before going on the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0005f7aaab2800f6170c399693a96917', '000920ad0b612851f8e01bcc880d9b3d', '0018ae58b01bdadc8e347995b69f99aa', '001c62abd11fa4b57bf7a6c603a11bb9', '001d865e65ef5d2579c190a0e0350d8f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['0005f7aaab2800f6170c399693a96917',\n",
       "  '000920ad0b612851f8e01bcc880d9b3d',\n",
       "  '0018ae58b01bdadc8e347995b69f99aa',\n",
       "  '001c62abd11fa4b57bf7a6c603a11bb9',\n",
       "  '001d865e65ef5d2579c190a0e0350d8f'],\n",
       " 10515)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_dir = [name.replace('_mask.tiff', '') for name in os.listdir(mask_dir)]\n",
    "id_dir.sort()\n",
    "print(id_dir[:5])\n",
    "\n",
    "id_dir.remove('3790f55cad63053e956fb73027179707')\n",
    "img_id_mutual_name = id_dir\n",
    "img_id_mutual_name[:5], len(img_id_mutual_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9790, 5, ['0005f7aaab2800f6170c399693a96917', 13312, 13824, 7168, 7680])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepdish as dd\n",
    "\n",
    "df = dd.io.load('/kaggle/input/h5-files/full_data_coordinate.h5')\n",
    "len(df)//36, len(df[0]), df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_mask(ID, coordinates, level = 1):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    data_img = skimage.io.MultiImage(os.path.join(data_dir, f'{ID}.tiff'))[level]\n",
    "    mask_img = skimage.io.MultiImage(os.path.join(mask_dir, f'{ID}_mask.tiff'))[level]\n",
    "    coordinates = [coordinate // 2**(2*level) for coordinate in coordinates]\n",
    "    data_tile = data_img[coordinates[0]: coordinates[1], coordinates[2]: coordinates[3], :]\n",
    "    mask_tile = mask_img[coordinates[0]: coordinates[1], coordinates[2]: coordinates[3], :]\n",
    "    data_tile = cv2.resize(data_tile, (512, 512))\n",
    "    mask_tile = cv2.resize(mask_tile, (512, 512))\n",
    "    del data_img, mask_img\n",
    "    \n",
    "    # Load and return small image\n",
    "    return data_tile, mask_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.7 ms, sys: 25.6 ms, total: 94.4 ms\n",
      "Wall time: 156 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 512, 512]), torch.Size([512, 512]), 352440)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class PANDADataset(Dataset):\n",
    "    def __init__(self, df, level = 2, transform=None):\n",
    "        self.df = df\n",
    "        self.level = level\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index, level = 2):\n",
    "        ID = self.df[index][0]\n",
    "        coordinate = self.df[index][1: ]\n",
    "        image, mask = load_data_and_mask(ID, coordinate, level)\n",
    "        \n",
    "        return torch.tensor(image).permute(2, 0, 1), torch.tensor(mask).permute(2, 0, 1)[0]\n",
    "    \n",
    "cls = PANDADataset(df, 2)\n",
    "%time cls[0][0].size(), cls[0][1].size(), len(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44055\n"
     ]
    }
   ],
   "source": [
    "dataLoader = DataLoader(cls, batch_size=8, shuffle=True, num_workers=8)\n",
    "del df, cls\n",
    "print(len(dataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://discuss.pytorch.org/t/unet-implementation/426\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,\n",
    "                 batch_norm=False, up_mode='upconv'):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n",
    "                                                padding, batch_norm))\n",
    "            prev_channels = 2**(wf+i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n",
    "                                            padding, batch_norm))\n",
    "            prev_channels = 2**(wf+i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path)-1:\n",
    "                blocks.append(x)\n",
    "                x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i-1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n",
    "                                         stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- unet params\n",
    "#these parameters get fed directly into the UNET class, and more description of them can be discovered there\n",
    "\n",
    "n_classes= 6    #number of classes in the data mask that we'll aim to predict\n",
    "\n",
    "\n",
    "in_channels= 3  #input channel of the data, RGB = 3\n",
    "padding= True   #should levels be padded\n",
    "depth= 5       #depth of the network \n",
    "wf= 2           #wf (int): number of filters in the first layer is 2**wf, was 6\n",
    "up_mode= 'upconv' #should we simply upsample the mask, or should we try and learn an interpolation \n",
    "batch_norm = True #should we use batch normalization between the layers\n",
    "\n",
    "# --- training params\n",
    "batch_size = 8\n",
    "patch_size = 512\n",
    "num_epochs = 5\n",
    "edge_weight = 1.1 #edges tend to be the most poorly segmented given how little area they occupy in the training set, this paramter boosts their values along the lines of the original UNET paper\n",
    "phases = [\"train\",\"val\"] #how many phases did we create databases for?\n",
    "validation_phases= [\"val\"] #when should we do valiation? note that validation is time consuming, so as opposed to doing for both training and validation, we do it only for vlaidation at the end of the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n"
     ]
    }
   ],
   "source": [
    "gpuid=0\n",
    "#specify if we should use a GPU (cuda) or only the CPU\n",
    "if(torch.cuda.is_available()):\n",
    "    print(torch.cuda.get_device_properties(gpuid))\n",
    "    torch.cuda.set_device(gpuid)\n",
    "    device = torch.device(f'cuda:{gpuid}')\n",
    "else:\n",
    "    device = torch.device(f'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \t122486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#build the model according to the paramters specified above and copy it to the GPU. finally print out the number of trainable parameters\n",
    "model = UNet(n_classes=n_classes, in_channels=in_channels, \n",
    "             padding=padding,depth=depth,wf=wf, \n",
    "             up_mode=up_mode, batch_norm=batch_norm).to(device)\n",
    "print(f\"total params: \\t{sum([np.prod(p.size()) for p in model.parameters()])}\")\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters()) #adam is going to be the most robust\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== Training started ========================================\n",
      "==================================================================================================\n",
      "Epoch 01, upto 00250 mini-batches; after 00 (minutes) and 59 (seconds);  train_loss = 1.683\n",
      "Epoch 01, upto 00500 mini-batches; after 01 (minutes) and 55 (seconds);  train_loss = 1.300\n",
      "Epoch 01, upto 00750 mini-batches; after 02 (minutes) and 52 (seconds);  train_loss = 1.074\n",
      "Epoch 01, upto 01000 mini-batches; after 03 (minutes) and 49 (seconds);  train_loss = 1.021\n",
      "Epoch 01, upto 01250 mini-batches; after 04 (minutes) and 47 (seconds);  train_loss = 1.021\n",
      "Epoch 01, upto 01500 mini-batches; after 05 (minutes) and 45 (seconds);  train_loss = 0.984\n",
      "Epoch 01, upto 01750 mini-batches; after 06 (minutes) and 42 (seconds);  train_loss = 0.989\n",
      "Epoch 01, upto 02000 mini-batches; after 07 (minutes) and 39 (seconds);  train_loss = 0.992\n",
      "Epoch 01, upto 02250 mini-batches; after 08 (minutes) and 37 (seconds);  train_loss = 0.976\n",
      "Epoch 01, upto 02500 mini-batches; after 09 (minutes) and 40 (seconds);  train_loss = 0.965\n",
      "Epoch 01, upto 02750 mini-batches; after 10 (minutes) and 37 (seconds);  train_loss = 0.977\n",
      "Epoch 01, upto 03000 mini-batches; after 11 (minutes) and 35 (seconds);  train_loss = 0.985\n",
      "Epoch 01, upto 03250 mini-batches; after 12 (minutes) and 33 (seconds);  train_loss = 0.965\n",
      "Epoch 01, upto 03500 mini-batches; after 13 (minutes) and 30 (seconds);  train_loss = 0.951\n",
      "Epoch 01, upto 03750 mini-batches; after 14 (minutes) and 26 (seconds);  train_loss = 0.984\n",
      "Epoch 01, upto 04000 mini-batches; after 15 (minutes) and 22 (seconds);  train_loss = 0.969\n",
      "Epoch 01, upto 04250 mini-batches; after 16 (minutes) and 21 (seconds);  train_loss = 0.931\n",
      "Epoch 01, upto 04500 mini-batches; after 17 (minutes) and 19 (seconds);  train_loss = 0.947\n",
      "Epoch 01, upto 04750 mini-batches; after 18 (minutes) and 16 (seconds);  train_loss = 0.943\n",
      "Epoch 01, upto 05000 mini-batches; after 19 (minutes) and 13 (seconds);  train_loss = 0.953\n",
      "Epoch 01, upto 05250 mini-batches; after 20 (minutes) and 12 (seconds);  train_loss = 0.966\n",
      "Epoch 01, upto 05500 mini-batches; after 21 (minutes) and 10 (seconds);  train_loss = 0.929\n",
      "Epoch 01, upto 05750 mini-batches; after 22 (minutes) and 09 (seconds);  train_loss = 0.923\n",
      "Epoch 01, upto 06000 mini-batches; after 23 (minutes) and 06 (seconds);  train_loss = 0.914\n",
      "Epoch 01, upto 06250 mini-batches; after 24 (minutes) and 03 (seconds);  train_loss = 0.922\n",
      "Epoch 01, upto 06500 mini-batches; after 25 (minutes) and 00 (seconds);  train_loss = 0.892\n",
      "Epoch 01, upto 06750 mini-batches; after 25 (minutes) and 57 (seconds);  train_loss = 0.931\n",
      "Epoch 01, upto 07000 mini-batches; after 26 (minutes) and 56 (seconds);  train_loss = 0.948\n",
      "Epoch 01, upto 07250 mini-batches; after 27 (minutes) and 53 (seconds);  train_loss = 0.917\n",
      "Epoch 01, upto 07500 mini-batches; after 28 (minutes) and 53 (seconds);  train_loss = 0.908\n",
      "Epoch 01, upto 07750 mini-batches; after 29 (minutes) and 50 (seconds);  train_loss = 0.900\n",
      "Epoch 01, upto 08000 mini-batches; after 30 (minutes) and 47 (seconds);  train_loss = 0.920\n",
      "Epoch 01, upto 08250 mini-batches; after 31 (minutes) and 45 (seconds);  train_loss = 0.897\n",
      "Epoch 01, upto 08500 mini-batches; after 32 (minutes) and 42 (seconds);  train_loss = 0.931\n",
      "Epoch 01, upto 08750 mini-batches; after 33 (minutes) and 39 (seconds);  train_loss = 0.899\n",
      "Epoch 01, upto 09000 mini-batches; after 34 (minutes) and 36 (seconds);  train_loss = 0.917\n",
      "Epoch 01, upto 09250 mini-batches; after 35 (minutes) and 34 (seconds);  train_loss = 0.906\n",
      "Epoch 01, upto 09500 mini-batches; after 36 (minutes) and 33 (seconds);  train_loss = 0.883\n",
      "Epoch 01, upto 09750 mini-batches; after 37 (minutes) and 29 (seconds);  train_loss = 0.891\n",
      "Epoch 01, upto 10000 mini-batches; after 38 (minutes) and 27 (seconds);  train_loss = 0.901\n",
      "Epoch 01, upto 10250 mini-batches; after 39 (minutes) and 24 (seconds);  train_loss = 0.888\n",
      "Epoch 01, upto 10500 mini-batches; after 40 (minutes) and 20 (seconds);  train_loss = 0.919\n",
      "Epoch 01, upto 10750 mini-batches; after 41 (minutes) and 19 (seconds);  train_loss = 0.891\n",
      "Epoch 01, upto 11000 mini-batches; after 42 (minutes) and 16 (seconds);  train_loss = 0.867\n",
      "Epoch 01, upto 11250 mini-batches; after 43 (minutes) and 14 (seconds);  train_loss = 0.870\n",
      "Epoch 01, upto 11500 mini-batches; after 44 (minutes) and 11 (seconds);  train_loss = 0.882\n",
      "Epoch 01, upto 11750 mini-batches; after 45 (minutes) and 09 (seconds);  train_loss = 0.885\n",
      "Epoch 01, upto 12000 mini-batches; after 46 (minutes) and 13 (seconds);  train_loss = 0.884\n",
      "Epoch 01, upto 12250 mini-batches; after 47 (minutes) and 11 (seconds);  train_loss = 0.905\n",
      "Epoch 01, upto 12500 mini-batches; after 48 (minutes) and 08 (seconds);  train_loss = 0.883\n",
      "Epoch 01, upto 12750 mini-batches; after 49 (minutes) and 05 (seconds);  train_loss = 0.859\n",
      "Epoch 01, upto 13000 mini-batches; after 50 (minutes) and 02 (seconds);  train_loss = 0.873\n",
      "Epoch 01, upto 13250 mini-batches; after 50 (minutes) and 59 (seconds);  train_loss = 0.876\n",
      "Epoch 01, upto 13500 mini-batches; after 51 (minutes) and 57 (seconds);  train_loss = 0.865\n",
      "Epoch 01, upto 13750 mini-batches; after 52 (minutes) and 55 (seconds);  train_loss = 0.893\n",
      "Epoch 01, upto 14000 mini-batches; after 53 (minutes) and 53 (seconds);  train_loss = 0.876\n",
      "Epoch 01, upto 14250 mini-batches; after 54 (minutes) and 50 (seconds);  train_loss = 0.888\n",
      "Epoch 01, upto 14500 mini-batches; after 55 (minutes) and 48 (seconds);  train_loss = 0.859\n",
      "Epoch 01, upto 14750 mini-batches; after 56 (minutes) and 46 (seconds);  train_loss = 0.848\n",
      "Epoch 01, upto 15000 mini-batches; after 57 (minutes) and 45 (seconds);  train_loss = 0.893\n",
      "Epoch 01, upto 15250 mini-batches; after 58 (minutes) and 44 (seconds);  train_loss = 0.860\n",
      "Epoch 01, upto 15500 mini-batches; after 59 (minutes) and 41 (seconds);  train_loss = 0.876\n",
      "Epoch 01, upto 15750 mini-batches; after 60 (minutes) and 39 (seconds);  train_loss = 0.884\n",
      "Epoch 01, upto 16000 mini-batches; after 61 (minutes) and 37 (seconds);  train_loss = 0.869\n",
      "Epoch 01, upto 16250 mini-batches; after 62 (minutes) and 35 (seconds);  train_loss = 0.864\n",
      "Epoch 01, upto 16500 mini-batches; after 63 (minutes) and 32 (seconds);  train_loss = 0.882\n",
      "Epoch 01, upto 16750 mini-batches; after 64 (minutes) and 30 (seconds);  train_loss = 0.887\n",
      "Epoch 01, upto 17000 mini-batches; after 65 (minutes) and 29 (seconds);  train_loss = 0.888\n",
      "Epoch 01, upto 17250 mini-batches; after 66 (minutes) and 26 (seconds);  train_loss = 0.868\n",
      "Epoch 01, upto 17500 mini-batches; after 67 (minutes) and 24 (seconds);  train_loss = 0.849\n",
      "Epoch 01, upto 17750 mini-batches; after 68 (minutes) and 22 (seconds);  train_loss = 0.869\n",
      "Epoch 01, upto 18000 mini-batches; after 69 (minutes) and 19 (seconds);  train_loss = 0.870\n",
      "Epoch 01, upto 18250 mini-batches; after 70 (minutes) and 18 (seconds);  train_loss = 0.862\n",
      "Epoch 01, upto 18500 mini-batches; after 71 (minutes) and 15 (seconds);  train_loss = 0.852\n",
      "Epoch 01, upto 18750 mini-batches; after 72 (minutes) and 14 (seconds);  train_loss = 0.853\n",
      "Epoch 01, upto 19000 mini-batches; after 73 (minutes) and 11 (seconds);  train_loss = 0.877\n",
      "Epoch 01, upto 19250 mini-batches; after 74 (minutes) and 09 (seconds);  train_loss = 0.896\n",
      "Epoch 01, upto 19500 mini-batches; after 75 (minutes) and 06 (seconds);  train_loss = 0.868\n",
      "Epoch 01, upto 19750 mini-batches; after 76 (minutes) and 04 (seconds);  train_loss = 0.849\n",
      "Epoch 01, upto 20000 mini-batches; after 77 (minutes) and 03 (seconds);  train_loss = 0.871\n",
      "Epoch 01, upto 20250 mini-batches; after 78 (minutes) and 01 (seconds);  train_loss = 0.859\n",
      "Epoch 01, upto 20500 mini-batches; after 78 (minutes) and 59 (seconds);  train_loss = 0.839\n",
      "Epoch 01, upto 20750 mini-batches; after 79 (minutes) and 57 (seconds);  train_loss = 0.840\n",
      "Epoch 01, upto 21000 mini-batches; after 80 (minutes) and 57 (seconds);  train_loss = 0.848\n",
      "Epoch 01, upto 21250 mini-batches; after 81 (minutes) and 55 (seconds);  train_loss = 0.861\n",
      "Epoch 01, upto 21500 mini-batches; after 82 (minutes) and 55 (seconds);  train_loss = 0.862\n",
      "Epoch 01, upto 21750 mini-batches; after 83 (minutes) and 53 (seconds);  train_loss = 0.855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, upto 22000 mini-batches; after 84 (minutes) and 52 (seconds);  train_loss = 0.838\n",
      "Epoch 01, upto 22250 mini-batches; after 85 (minutes) and 50 (seconds);  train_loss = 0.854\n",
      "Epoch 01, upto 22500 mini-batches; after 86 (minutes) and 49 (seconds);  train_loss = 0.837\n",
      "Epoch 01, upto 22750 mini-batches; after 87 (minutes) and 48 (seconds);  train_loss = 0.865\n",
      "Epoch 01, upto 23000 mini-batches; after 88 (minutes) and 47 (seconds);  train_loss = 0.857\n",
      "Epoch 01, upto 23250 mini-batches; after 89 (minutes) and 45 (seconds);  train_loss = 0.845\n",
      "Epoch 01, upto 23500 mini-batches; after 90 (minutes) and 46 (seconds);  train_loss = 0.836\n",
      "Epoch 01, upto 23750 mini-batches; after 91 (minutes) and 45 (seconds);  train_loss = 0.854\n",
      "Epoch 01, upto 24000 mini-batches; after 92 (minutes) and 44 (seconds);  train_loss = 0.861\n",
      "Epoch 01, upto 24250 mini-batches; after 93 (minutes) and 44 (seconds);  train_loss = 0.823\n",
      "Epoch 01, upto 24500 mini-batches; after 94 (minutes) and 43 (seconds);  train_loss = 0.841\n",
      "Epoch 01, upto 24750 mini-batches; after 95 (minutes) and 41 (seconds);  train_loss = 0.862\n",
      "Epoch 01, upto 25000 mini-batches; after 96 (minutes) and 40 (seconds);  train_loss = 0.846\n",
      "Epoch 01, upto 25250 mini-batches; after 97 (minutes) and 38 (seconds);  train_loss = 0.855\n",
      "Epoch 01, upto 25500 mini-batches; after 98 (minutes) and 36 (seconds);  train_loss = 0.853\n",
      "Epoch 01, upto 25750 mini-batches; after 99 (minutes) and 35 (seconds);  train_loss = 0.856\n",
      "Epoch 01, upto 26000 mini-batches; after 100 (minutes) and 34 (seconds);  train_loss = 0.850\n",
      "Epoch 01, upto 26250 mini-batches; after 101 (minutes) and 33 (seconds);  train_loss = 0.849\n",
      "Epoch 01, upto 26500 mini-batches; after 102 (minutes) and 31 (seconds);  train_loss = 0.826\n",
      "Epoch 01, upto 26750 mini-batches; after 103 (minutes) and 31 (seconds);  train_loss = 0.877\n",
      "Epoch 01, upto 27000 mini-batches; after 104 (minutes) and 29 (seconds);  train_loss = 0.850\n",
      "Epoch 01, upto 27250 mini-batches; after 105 (minutes) and 26 (seconds);  train_loss = 0.842\n",
      "Epoch 01, upto 27500 mini-batches; after 106 (minutes) and 26 (seconds);  train_loss = 0.845\n",
      "Epoch 01, upto 27750 mini-batches; after 107 (minutes) and 24 (seconds);  train_loss = 0.845\n",
      "Epoch 01, upto 28000 mini-batches; after 108 (minutes) and 22 (seconds);  train_loss = 0.861\n",
      "Epoch 01, upto 28250 mini-batches; after 109 (minutes) and 21 (seconds);  train_loss = 0.852\n",
      "Epoch 01, upto 28500 mini-batches; after 110 (minutes) and 20 (seconds);  train_loss = 0.842\n",
      "Epoch 01, upto 28750 mini-batches; after 111 (minutes) and 23 (seconds);  train_loss = 0.836\n",
      "Epoch 01, upto 29000 mini-batches; after 112 (minutes) and 20 (seconds);  train_loss = 0.829\n",
      "Epoch 01, upto 29250 mini-batches; after 113 (minutes) and 19 (seconds);  train_loss = 0.831\n",
      "Epoch 01, upto 29500 mini-batches; after 114 (minutes) and 18 (seconds);  train_loss = 0.832\n",
      "Epoch 01, upto 29750 mini-batches; after 115 (minutes) and 16 (seconds);  train_loss = 0.829\n",
      "Epoch 01, upto 30000 mini-batches; after 116 (minutes) and 16 (seconds);  train_loss = 0.848\n",
      "Epoch 01, upto 30250 mini-batches; after 117 (minutes) and 14 (seconds);  train_loss = 0.860\n",
      "Epoch 01, upto 30500 mini-batches; after 118 (minutes) and 13 (seconds);  train_loss = 0.829\n",
      "Epoch 01, upto 30750 mini-batches; after 119 (minutes) and 11 (seconds);  train_loss = 0.841\n",
      "Epoch 01, upto 31000 mini-batches; after 120 (minutes) and 10 (seconds);  train_loss = 0.843\n",
      "Epoch 01, upto 31250 mini-batches; after 121 (minutes) and 10 (seconds);  train_loss = 0.841\n",
      "Epoch 01, upto 31500 mini-batches; after 122 (minutes) and 08 (seconds);  train_loss = 0.848\n",
      "Epoch 01, upto 31750 mini-batches; after 123 (minutes) and 07 (seconds);  train_loss = 0.843\n",
      "Epoch 01, upto 32000 mini-batches; after 124 (minutes) and 05 (seconds);  train_loss = 0.826\n",
      "Epoch 01, upto 32250 mini-batches; after 125 (minutes) and 03 (seconds);  train_loss = 0.834\n",
      "Epoch 01, upto 32500 mini-batches; after 126 (minutes) and 02 (seconds);  train_loss = 0.863\n",
      "Epoch 01, upto 32750 mini-batches; after 127 (minutes) and 01 (seconds);  train_loss = 0.827\n",
      "Epoch 01, upto 33000 mini-batches; after 127 (minutes) and 59 (seconds);  train_loss = 0.859\n",
      "Epoch 01, upto 33250 mini-batches; after 128 (minutes) and 58 (seconds);  train_loss = 0.844\n",
      "Epoch 01, upto 33500 mini-batches; after 129 (minutes) and 57 (seconds);  train_loss = 0.846\n",
      "Epoch 01, upto 33750 mini-batches; after 130 (minutes) and 57 (seconds);  train_loss = 0.834\n",
      "Epoch 01, upto 34000 mini-batches; after 131 (minutes) and 56 (seconds);  train_loss = 0.860\n",
      "Epoch 01, upto 34250 mini-batches; after 132 (minutes) and 54 (seconds);  train_loss = 0.818\n",
      "Epoch 01, upto 34500 mini-batches; after 133 (minutes) and 53 (seconds);  train_loss = 0.807\n",
      "Epoch 01, upto 34750 mini-batches; after 134 (minutes) and 53 (seconds);  train_loss = 0.844\n",
      "Epoch 01, upto 35000 mini-batches; after 135 (minutes) and 50 (seconds);  train_loss = 0.837\n",
      "Epoch 01, upto 35250 mini-batches; after 136 (minutes) and 49 (seconds);  train_loss = 0.845\n",
      "Epoch 01, upto 35500 mini-batches; after 137 (minutes) and 48 (seconds);  train_loss = 0.840\n",
      "Epoch 01, upto 35750 mini-batches; after 138 (minutes) and 48 (seconds);  train_loss = 0.846\n",
      "Epoch 01, upto 36000 mini-batches; after 139 (minutes) and 47 (seconds);  train_loss = 0.842\n",
      "Epoch 01, upto 36250 mini-batches; after 140 (minutes) and 46 (seconds);  train_loss = 0.832\n",
      "Epoch 01, upto 36500 mini-batches; after 141 (minutes) and 45 (seconds);  train_loss = 0.844\n",
      "Epoch 01, upto 36750 mini-batches; after 142 (minutes) and 44 (seconds);  train_loss = 0.842\n",
      "Epoch 01, upto 37000 mini-batches; after 143 (minutes) and 43 (seconds);  train_loss = 0.823\n",
      "Epoch 01, upto 37250 mini-batches; after 144 (minutes) and 41 (seconds);  train_loss = 0.839\n",
      "Epoch 01, upto 37500 mini-batches; after 145 (minutes) and 41 (seconds);  train_loss = 0.847\n",
      "Epoch 01, upto 37750 mini-batches; after 146 (minutes) and 40 (seconds);  train_loss = 0.810\n",
      "Epoch 01, upto 38000 mini-batches; after 147 (minutes) and 39 (seconds);  train_loss = 0.845\n",
      "Epoch 01, upto 38250 mini-batches; after 148 (minutes) and 38 (seconds);  train_loss = 0.829\n",
      "Epoch 01, upto 38500 mini-batches; after 149 (minutes) and 37 (seconds);  train_loss = 0.840\n",
      "Epoch 01, upto 38750 mini-batches; after 150 (minutes) and 37 (seconds);  train_loss = 0.832\n",
      "Epoch 01, upto 39000 mini-batches; after 151 (minutes) and 36 (seconds);  train_loss = 0.836\n",
      "Epoch 01, upto 39250 mini-batches; after 152 (minutes) and 36 (seconds);  train_loss = 0.810\n",
      "Epoch 01, upto 39500 mini-batches; after 153 (minutes) and 36 (seconds);  train_loss = 0.832\n",
      "Epoch 01, upto 39750 mini-batches; after 154 (minutes) and 34 (seconds);  train_loss = 0.821\n",
      "Epoch 01, upto 40000 mini-batches; after 155 (minutes) and 33 (seconds);  train_loss = 0.841\n",
      "Epoch 01, upto 40250 mini-batches; after 156 (minutes) and 33 (seconds);  train_loss = 0.838\n",
      "Epoch 01, upto 40500 mini-batches; after 157 (minutes) and 32 (seconds);  train_loss = 0.828\n",
      "Epoch 01, upto 40750 mini-batches; after 158 (minutes) and 31 (seconds);  train_loss = 0.816\n",
      "Epoch 01, upto 41000 mini-batches; after 159 (minutes) and 29 (seconds);  train_loss = 0.834\n",
      "Epoch 01, upto 41250 mini-batches; after 160 (minutes) and 29 (seconds);  train_loss = 0.827\n",
      "Epoch 01, upto 41500 mini-batches; after 161 (minutes) and 28 (seconds);  train_loss = 0.828\n",
      "Epoch 01, upto 41750 mini-batches; after 162 (minutes) and 27 (seconds);  train_loss = 0.834\n",
      "Epoch 01, upto 42000 mini-batches; after 163 (minutes) and 25 (seconds);  train_loss = 0.833\n",
      "Epoch 01, upto 42250 mini-batches; after 164 (minutes) and 24 (seconds);  train_loss = 0.830\n",
      "Epoch 01, upto 42500 mini-batches; after 165 (minutes) and 24 (seconds);  train_loss = 0.831\n",
      "Epoch 01, upto 42750 mini-batches; after 166 (minutes) and 22 (seconds);  train_loss = 0.860\n",
      "Epoch 01, upto 43000 mini-batches; after 167 (minutes) and 22 (seconds);  train_loss = 0.841\n",
      "Epoch 01, upto 43250 mini-batches; after 168 (minutes) and 20 (seconds);  train_loss = 0.813\n",
      "Epoch 01, upto 43500 mini-batches; after 169 (minutes) and 19 (seconds);  train_loss = 0.840\n",
      "Epoch 01, upto 43750 mini-batches; after 170 (minutes) and 19 (seconds);  train_loss = 0.828\n",
      "Epoch 01, upto 44000 mini-batches; after 171 (minutes) and 18 (seconds);  train_loss = 0.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Epoch 02, upto 00250 mini-batches; after 01 (minutes) and 00 (seconds);  train_loss = 0.850\n",
      "Epoch 02, upto 00500 mini-batches; after 02 (minutes) and 00 (seconds);  train_loss = 0.818\n",
      "Epoch 02, upto 00750 mini-batches; after 03 (minutes) and 00 (seconds);  train_loss = 0.835\n",
      "Epoch 02, upto 01000 mini-batches; after 03 (minutes) and 58 (seconds);  train_loss = 0.837\n",
      "Epoch 02, upto 01250 mini-batches; after 04 (minutes) and 56 (seconds);  train_loss = 0.828\n",
      "Epoch 02, upto 01500 mini-batches; after 05 (minutes) and 57 (seconds);  train_loss = 0.849\n",
      "Epoch 02, upto 01750 mini-batches; after 06 (minutes) and 57 (seconds);  train_loss = 0.844\n",
      "Epoch 02, upto 02000 mini-batches; after 07 (minutes) and 56 (seconds);  train_loss = 0.819\n",
      "Epoch 02, upto 02250 mini-batches; after 08 (minutes) and 54 (seconds);  train_loss = 0.825\n",
      "Epoch 02, upto 02500 mini-batches; after 09 (minutes) and 55 (seconds);  train_loss = 0.825\n",
      "Epoch 02, upto 02750 mini-batches; after 10 (minutes) and 54 (seconds);  train_loss = 0.836\n",
      "Epoch 02, upto 03000 mini-batches; after 11 (minutes) and 53 (seconds);  train_loss = 0.819\n",
      "Epoch 02, upto 03250 mini-batches; after 12 (minutes) and 51 (seconds);  train_loss = 0.838\n",
      "Epoch 02, upto 03500 mini-batches; after 13 (minutes) and 50 (seconds);  train_loss = 0.841\n",
      "Epoch 02, upto 03750 mini-batches; after 14 (minutes) and 50 (seconds);  train_loss = 0.814\n",
      "Epoch 02, upto 04000 mini-batches; after 15 (minutes) and 49 (seconds);  train_loss = 0.839\n",
      "Epoch 02, upto 04250 mini-batches; after 16 (minutes) and 47 (seconds);  train_loss = 0.844\n",
      "Epoch 02, upto 04500 mini-batches; after 17 (minutes) and 46 (seconds);  train_loss = 0.819\n",
      "Epoch 02, upto 04750 mini-batches; after 18 (minutes) and 46 (seconds);  train_loss = 0.835\n",
      "Epoch 02, upto 05000 mini-batches; after 19 (minutes) and 44 (seconds);  train_loss = 0.828\n",
      "Epoch 02, upto 05250 mini-batches; after 20 (minutes) and 43 (seconds);  train_loss = 0.814\n",
      "Epoch 02, upto 05500 mini-batches; after 21 (minutes) and 41 (seconds);  train_loss = 0.823\n",
      "Epoch 02, upto 05750 mini-batches; after 22 (minutes) and 40 (seconds);  train_loss = 0.828\n",
      "Epoch 02, upto 06000 mini-batches; after 23 (minutes) and 39 (seconds);  train_loss = 0.813\n",
      "Epoch 02, upto 06250 mini-batches; after 24 (minutes) and 37 (seconds);  train_loss = 0.847\n",
      "Epoch 02, upto 06500 mini-batches; after 25 (minutes) and 35 (seconds);  train_loss = 0.828\n",
      "Epoch 02, upto 06750 mini-batches; after 26 (minutes) and 33 (seconds);  train_loss = 0.822\n",
      "Epoch 02, upto 07000 mini-batches; after 27 (minutes) and 31 (seconds);  train_loss = 0.835\n",
      "Epoch 02, upto 07250 mini-batches; after 28 (minutes) and 29 (seconds);  train_loss = 0.801\n",
      "Epoch 02, upto 07500 mini-batches; after 29 (minutes) and 28 (seconds);  train_loss = 0.852\n",
      "Epoch 02, upto 07750 mini-batches; after 30 (minutes) and 28 (seconds);  train_loss = 0.842\n",
      "Epoch 02, upto 08000 mini-batches; after 31 (minutes) and 26 (seconds);  train_loss = 0.823\n",
      "Epoch 02, upto 08250 mini-batches; after 32 (minutes) and 24 (seconds);  train_loss = 0.833\n",
      "Epoch 02, upto 08500 mini-batches; after 33 (minutes) and 23 (seconds);  train_loss = 0.833\n",
      "Epoch 02, upto 08750 mini-batches; after 34 (minutes) and 20 (seconds);  train_loss = 0.835\n",
      "Epoch 02, upto 09000 mini-batches; after 35 (minutes) and 18 (seconds);  train_loss = 0.826\n",
      "Epoch 02, upto 09250 mini-batches; after 36 (minutes) and 17 (seconds);  train_loss = 0.821\n",
      "Epoch 02, upto 09500 mini-batches; after 37 (minutes) and 16 (seconds);  train_loss = 0.821\n",
      "Epoch 02, upto 09750 mini-batches; after 38 (minutes) and 15 (seconds);  train_loss = 0.813\n",
      "Epoch 02, upto 10000 mini-batches; after 39 (minutes) and 13 (seconds);  train_loss = 0.836\n",
      "Epoch 02, upto 10250 mini-batches; after 40 (minutes) and 12 (seconds);  train_loss = 0.822\n",
      "Epoch 02, upto 10500 mini-batches; after 41 (minutes) and 10 (seconds);  train_loss = 0.792\n",
      "Epoch 02, upto 10750 mini-batches; after 42 (minutes) and 07 (seconds);  train_loss = 0.840\n",
      "Epoch 02, upto 11000 mini-batches; after 43 (minutes) and 06 (seconds);  train_loss = 0.839\n",
      "Epoch 02, upto 11250 mini-batches; after 44 (minutes) and 04 (seconds);  train_loss = 0.842\n",
      "Epoch 02, upto 11500 mini-batches; after 45 (minutes) and 02 (seconds);  train_loss = 0.823\n",
      "Epoch 02, upto 11750 mini-batches; after 46 (minutes) and 00 (seconds);  train_loss = 0.818\n",
      "Epoch 02, upto 12000 mini-batches; after 46 (minutes) and 58 (seconds);  train_loss = 0.802\n",
      "Epoch 02, upto 12250 mini-batches; after 47 (minutes) and 56 (seconds);  train_loss = 0.828\n",
      "Epoch 02, upto 12500 mini-batches; after 48 (minutes) and 54 (seconds);  train_loss = 0.797\n",
      "Epoch 02, upto 12750 mini-batches; after 49 (minutes) and 52 (seconds);  train_loss = 0.804\n",
      "Epoch 02, upto 13000 mini-batches; after 50 (minutes) and 49 (seconds);  train_loss = 0.805\n",
      "Epoch 02, upto 13250 mini-batches; after 51 (minutes) and 47 (seconds);  train_loss = 0.831\n",
      "Epoch 02, upto 13500 mini-batches; after 52 (minutes) and 45 (seconds);  train_loss = 0.835\n",
      "Epoch 02, upto 13750 mini-batches; after 53 (minutes) and 45 (seconds);  train_loss = 0.814\n",
      "Epoch 02, upto 14000 mini-batches; after 54 (minutes) and 43 (seconds);  train_loss = 0.834\n",
      "Epoch 02, upto 14250 mini-batches; after 55 (minutes) and 43 (seconds);  train_loss = 0.829\n",
      "Epoch 02, upto 14500 mini-batches; after 56 (minutes) and 41 (seconds);  train_loss = 0.815\n",
      "Epoch 02, upto 14750 mini-batches; after 57 (minutes) and 39 (seconds);  train_loss = 0.809\n",
      "Epoch 02, upto 15000 mini-batches; after 58 (minutes) and 39 (seconds);  train_loss = 0.833\n",
      "Epoch 02, upto 15250 mini-batches; after 59 (minutes) and 36 (seconds);  train_loss = 0.807\n",
      "Epoch 02, upto 15500 mini-batches; after 60 (minutes) and 34 (seconds);  train_loss = 0.834\n",
      "Epoch 02, upto 15750 mini-batches; after 61 (minutes) and 32 (seconds);  train_loss = 0.802\n",
      "Epoch 02, upto 16000 mini-batches; after 62 (minutes) and 31 (seconds);  train_loss = 0.820\n",
      "Epoch 02, upto 16250 mini-batches; after 63 (minutes) and 28 (seconds);  train_loss = 0.825\n",
      "Epoch 02, upto 16500 mini-batches; after 64 (minutes) and 26 (seconds);  train_loss = 0.802\n",
      "Epoch 02, upto 16750 mini-batches; after 65 (minutes) and 26 (seconds);  train_loss = 0.803\n",
      "Epoch 02, upto 17000 mini-batches; after 66 (minutes) and 24 (seconds);  train_loss = 0.806\n",
      "Epoch 02, upto 17250 mini-batches; after 67 (minutes) and 22 (seconds);  train_loss = 0.834\n",
      "Epoch 02, upto 17500 mini-batches; after 68 (minutes) and 22 (seconds);  train_loss = 0.828\n",
      "Epoch 02, upto 17750 mini-batches; after 69 (minutes) and 21 (seconds);  train_loss = 0.835\n",
      "Epoch 02, upto 18000 mini-batches; after 70 (minutes) and 22 (seconds);  train_loss = 0.831\n",
      "Epoch 02, upto 18250 mini-batches; after 71 (minutes) and 20 (seconds);  train_loss = 0.821\n",
      "Epoch 02, upto 18500 mini-batches; after 72 (minutes) and 18 (seconds);  train_loss = 0.834\n",
      "Epoch 02, upto 18750 mini-batches; after 73 (minutes) and 15 (seconds);  train_loss = 0.840\n",
      "Epoch 02, upto 19000 mini-batches; after 74 (minutes) and 14 (seconds);  train_loss = 0.818\n",
      "Epoch 02, upto 19250 mini-batches; after 75 (minutes) and 11 (seconds);  train_loss = 0.823\n",
      "Epoch 02, upto 19500 mini-batches; after 76 (minutes) and 18 (seconds);  train_loss = 0.816\n",
      "Epoch 02, upto 19750 mini-batches; after 77 (minutes) and 16 (seconds);  train_loss = 0.818\n",
      "Epoch 02, upto 20000 mini-batches; after 78 (minutes) and 14 (seconds);  train_loss = 0.820\n",
      "Epoch 02, upto 20250 mini-batches; after 79 (minutes) and 12 (seconds);  train_loss = 0.814\n",
      "Epoch 02, upto 20500 mini-batches; after 80 (minutes) and 10 (seconds);  train_loss = 0.808\n",
      "Epoch 02, upto 20750 mini-batches; after 81 (minutes) and 08 (seconds);  train_loss = 0.815\n",
      "Epoch 02, upto 21000 mini-batches; after 82 (minutes) and 06 (seconds);  train_loss = 0.828\n",
      "Epoch 02, upto 21250 mini-batches; after 83 (minutes) and 03 (seconds);  train_loss = 0.815\n",
      "Epoch 02, upto 21500 mini-batches; after 83 (minutes) and 59 (seconds);  train_loss = 0.834\n",
      "Epoch 02, upto 21750 mini-batches; after 84 (minutes) and 57 (seconds);  train_loss = 0.804\n",
      "Epoch 02, upto 22000 mini-batches; after 85 (minutes) and 55 (seconds);  train_loss = 0.815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02, upto 22250 mini-batches; after 86 (minutes) and 54 (seconds);  train_loss = 0.817\n",
      "Epoch 02, upto 22500 mini-batches; after 87 (minutes) and 51 (seconds);  train_loss = 0.835\n",
      "Epoch 02, upto 22750 mini-batches; after 88 (minutes) and 49 (seconds);  train_loss = 0.802\n",
      "Epoch 02, upto 23000 mini-batches; after 89 (minutes) and 48 (seconds);  train_loss = 0.823\n",
      "Epoch 02, upto 23250 mini-batches; after 90 (minutes) and 46 (seconds);  train_loss = 0.820\n",
      "Epoch 02, upto 23500 mini-batches; after 91 (minutes) and 44 (seconds);  train_loss = 0.819\n",
      "Epoch 02, upto 23750 mini-batches; after 92 (minutes) and 42 (seconds);  train_loss = 0.817\n",
      "Epoch 02, upto 24000 mini-batches; after 93 (minutes) and 40 (seconds);  train_loss = 0.819\n",
      "Epoch 02, upto 24250 mini-batches; after 94 (minutes) and 37 (seconds);  train_loss = 0.798\n",
      "Epoch 02, upto 24500 mini-batches; after 95 (minutes) and 35 (seconds);  train_loss = 0.824\n",
      "Epoch 02, upto 24750 mini-batches; after 96 (minutes) and 34 (seconds);  train_loss = 0.808\n",
      "Epoch 02, upto 25000 mini-batches; after 97 (minutes) and 32 (seconds);  train_loss = 0.830\n",
      "Epoch 02, upto 25250 mini-batches; after 98 (minutes) and 32 (seconds);  train_loss = 0.780\n",
      "Epoch 02, upto 25500 mini-batches; after 99 (minutes) and 29 (seconds);  train_loss = 0.835\n",
      "Epoch 02, upto 25750 mini-batches; after 100 (minutes) and 27 (seconds);  train_loss = 0.805\n",
      "Epoch 02, upto 26000 mini-batches; after 101 (minutes) and 26 (seconds);  train_loss = 0.823\n",
      "Epoch 02, upto 26250 mini-batches; after 102 (minutes) and 24 (seconds);  train_loss = 0.823\n",
      "Epoch 02, upto 26500 mini-batches; after 103 (minutes) and 21 (seconds);  train_loss = 0.824\n",
      "Epoch 02, upto 26750 mini-batches; after 104 (minutes) and 20 (seconds);  train_loss = 0.815\n",
      "Epoch 02, upto 27000 mini-batches; after 105 (minutes) and 17 (seconds);  train_loss = 0.792\n",
      "Epoch 02, upto 27250 mini-batches; after 106 (minutes) and 15 (seconds);  train_loss = 0.804\n",
      "Epoch 02, upto 27500 mini-batches; after 107 (minutes) and 13 (seconds);  train_loss = 0.799\n",
      "Epoch 02, upto 27750 mini-batches; after 108 (minutes) and 11 (seconds);  train_loss = 0.818\n",
      "Epoch 02, upto 28000 mini-batches; after 109 (minutes) and 10 (seconds);  train_loss = 0.793\n",
      "Epoch 02, upto 28250 mini-batches; after 110 (minutes) and 07 (seconds);  train_loss = 0.824\n",
      "Epoch 02, upto 28500 mini-batches; after 111 (minutes) and 06 (seconds);  train_loss = 0.822\n",
      "Epoch 02, upto 28750 mini-batches; after 112 (minutes) and 04 (seconds);  train_loss = 0.815\n",
      "Epoch 02, upto 29000 mini-batches; after 113 (minutes) and 02 (seconds);  train_loss = 0.840\n",
      "Epoch 02, upto 29250 mini-batches; after 114 (minutes) and 00 (seconds);  train_loss = 0.822\n",
      "Epoch 02, upto 29500 mini-batches; after 114 (minutes) and 59 (seconds);  train_loss = 0.815\n",
      "Epoch 02, upto 29750 mini-batches; after 115 (minutes) and 56 (seconds);  train_loss = 0.815\n",
      "Epoch 02, upto 30000 mini-batches; after 116 (minutes) and 54 (seconds);  train_loss = 0.819\n",
      "Epoch 02, upto 30250 mini-batches; after 117 (minutes) and 51 (seconds);  train_loss = 0.828\n",
      "Epoch 02, upto 30500 mini-batches; after 118 (minutes) and 50 (seconds);  train_loss = 0.808\n",
      "Epoch 02, upto 30750 mini-batches; after 119 (minutes) and 47 (seconds);  train_loss = 0.808\n",
      "Epoch 02, upto 31000 mini-batches; after 120 (minutes) and 46 (seconds);  train_loss = 0.797\n",
      "Epoch 02, upto 31250 mini-batches; after 121 (minutes) and 43 (seconds);  train_loss = 0.832\n",
      "Epoch 02, upto 31500 mini-batches; after 122 (minutes) and 43 (seconds);  train_loss = 0.811\n",
      "Epoch 02, upto 31750 mini-batches; after 123 (minutes) and 40 (seconds);  train_loss = 0.817\n",
      "Epoch 02, upto 32000 mini-batches; after 124 (minutes) and 39 (seconds);  train_loss = 0.831\n",
      "Epoch 02, upto 32250 mini-batches; after 125 (minutes) and 37 (seconds);  train_loss = 0.811\n",
      "Epoch 02, upto 32500 mini-batches; after 126 (minutes) and 35 (seconds);  train_loss = 0.802\n",
      "Epoch 02, upto 32750 mini-batches; after 127 (minutes) and 33 (seconds);  train_loss = 0.829\n",
      "Epoch 02, upto 33000 mini-batches; after 128 (minutes) and 31 (seconds);  train_loss = 0.822\n",
      "Epoch 02, upto 33250 mini-batches; after 129 (minutes) and 30 (seconds);  train_loss = 0.824\n",
      "Epoch 02, upto 33500 mini-batches; after 130 (minutes) and 28 (seconds);  train_loss = 0.827\n",
      "Epoch 02, upto 33750 mini-batches; after 131 (minutes) and 26 (seconds);  train_loss = 0.826\n",
      "Epoch 02, upto 34000 mini-batches; after 132 (minutes) and 24 (seconds);  train_loss = 0.802\n",
      "Epoch 02, upto 34250 mini-batches; after 133 (minutes) and 24 (seconds);  train_loss = 0.820\n",
      "Epoch 02, upto 34500 mini-batches; after 134 (minutes) and 23 (seconds);  train_loss = 0.818\n",
      "Epoch 02, upto 34750 mini-batches; after 135 (minutes) and 21 (seconds);  train_loss = 0.815\n",
      "Epoch 02, upto 35000 mini-batches; after 136 (minutes) and 20 (seconds);  train_loss = 0.817\n",
      "Epoch 02, upto 35250 mini-batches; after 137 (minutes) and 18 (seconds);  train_loss = 0.841\n",
      "Epoch 02, upto 35500 mini-batches; after 138 (minutes) and 16 (seconds);  train_loss = 0.807\n",
      "Epoch 02, upto 35750 mini-batches; after 139 (minutes) and 15 (seconds);  train_loss = 0.797\n",
      "Epoch 02, upto 36000 mini-batches; after 140 (minutes) and 14 (seconds);  train_loss = 0.842\n",
      "Epoch 02, upto 36250 mini-batches; after 141 (minutes) and 12 (seconds);  train_loss = 0.807\n",
      "Epoch 02, upto 36500 mini-batches; after 142 (minutes) and 11 (seconds);  train_loss = 0.832\n",
      "Epoch 02, upto 36750 mini-batches; after 143 (minutes) and 09 (seconds);  train_loss = 0.803\n",
      "Epoch 02, upto 37000 mini-batches; after 144 (minutes) and 08 (seconds);  train_loss = 0.828\n",
      "Epoch 02, upto 37250 mini-batches; after 145 (minutes) and 06 (seconds);  train_loss = 0.814\n",
      "Epoch 02, upto 37500 mini-batches; after 146 (minutes) and 05 (seconds);  train_loss = 0.785\n",
      "Epoch 02, upto 37750 mini-batches; after 147 (minutes) and 04 (seconds);  train_loss = 0.824\n",
      "Epoch 02, upto 38000 mini-batches; after 148 (minutes) and 02 (seconds);  train_loss = 0.800\n",
      "Epoch 02, upto 38250 mini-batches; after 149 (minutes) and 00 (seconds);  train_loss = 0.827\n",
      "Epoch 02, upto 38500 mini-batches; after 150 (minutes) and 00 (seconds);  train_loss = 0.826\n",
      "Epoch 02, upto 38750 mini-batches; after 150 (minutes) and 58 (seconds);  train_loss = 0.802\n",
      "Epoch 02, upto 39000 mini-batches; after 151 (minutes) and 55 (seconds);  train_loss = 0.812\n",
      "Epoch 02, upto 39250 mini-batches; after 152 (minutes) and 54 (seconds);  train_loss = 0.821\n",
      "Epoch 02, upto 39500 mini-batches; after 153 (minutes) and 53 (seconds);  train_loss = 0.815\n",
      "Epoch 02, upto 39750 mini-batches; after 154 (minutes) and 50 (seconds);  train_loss = 0.803\n",
      "Epoch 02, upto 40000 mini-batches; after 155 (minutes) and 48 (seconds);  train_loss = 0.803\n",
      "Epoch 02, upto 40250 mini-batches; after 156 (minutes) and 46 (seconds);  train_loss = 0.800\n",
      "Epoch 02, upto 40500 mini-batches; after 157 (minutes) and 44 (seconds);  train_loss = 0.801\n",
      "Epoch 02, upto 40750 mini-batches; after 158 (minutes) and 44 (seconds);  train_loss = 0.854\n",
      "Epoch 02, upto 41000 mini-batches; after 159 (minutes) and 42 (seconds);  train_loss = 0.803\n",
      "Epoch 02, upto 41250 mini-batches; after 160 (minutes) and 40 (seconds);  train_loss = 0.791\n",
      "Epoch 02, upto 41500 mini-batches; after 161 (minutes) and 39 (seconds);  train_loss = 0.805\n",
      "Epoch 02, upto 41750 mini-batches; after 162 (minutes) and 38 (seconds);  train_loss = 0.805\n",
      "Epoch 02, upto 42000 mini-batches; after 163 (minutes) and 36 (seconds);  train_loss = 0.816\n",
      "Epoch 02, upto 42250 mini-batches; after 164 (minutes) and 34 (seconds);  train_loss = 0.812\n",
      "Epoch 02, upto 42500 mini-batches; after 165 (minutes) and 32 (seconds);  train_loss = 0.797\n",
      "Epoch 02, upto 42750 mini-batches; after 166 (minutes) and 31 (seconds);  train_loss = 0.798\n",
      "Epoch 02, upto 43000 mini-batches; after 167 (minutes) and 30 (seconds);  train_loss = 0.796\n",
      "Epoch 02, upto 43250 mini-batches; after 168 (minutes) and 29 (seconds);  train_loss = 0.818\n",
      "Epoch 02, upto 43500 mini-batches; after 169 (minutes) and 28 (seconds);  train_loss = 0.820\n",
      "Epoch 02, upto 43750 mini-batches; after 170 (minutes) and 26 (seconds);  train_loss = 0.818\n",
      "Epoch 02, upto 44000 mini-batches; after 171 (minutes) and 23 (seconds);  train_loss = 0.811\n",
      "==================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03, upto 00250 mini-batches; after 00 (minutes) and 59 (seconds);  train_loss = 0.819\n",
      "Epoch 03, upto 00500 mini-batches; after 01 (minutes) and 58 (seconds);  train_loss = 0.822\n",
      "Epoch 03, upto 00750 mini-batches; after 02 (minutes) and 56 (seconds);  train_loss = 0.813\n",
      "Epoch 03, upto 01000 mini-batches; after 03 (minutes) and 54 (seconds);  train_loss = 0.835\n",
      "Epoch 03, upto 01250 mini-batches; after 04 (minutes) and 52 (seconds);  train_loss = 0.824\n",
      "Epoch 03, upto 01500 mini-batches; after 05 (minutes) and 51 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 01750 mini-batches; after 06 (minutes) and 49 (seconds);  train_loss = 0.802\n",
      "Epoch 03, upto 02000 mini-batches; after 07 (minutes) and 48 (seconds);  train_loss = 0.814\n",
      "Epoch 03, upto 02250 mini-batches; after 08 (minutes) and 46 (seconds);  train_loss = 0.786\n",
      "Epoch 03, upto 02500 mini-batches; after 09 (minutes) and 44 (seconds);  train_loss = 0.791\n",
      "Epoch 03, upto 02750 mini-batches; after 10 (minutes) and 42 (seconds);  train_loss = 0.812\n",
      "Epoch 03, upto 03000 mini-batches; after 11 (minutes) and 41 (seconds);  train_loss = 0.783\n",
      "Epoch 03, upto 03250 mini-batches; after 12 (minutes) and 40 (seconds);  train_loss = 0.814\n",
      "Epoch 03, upto 03500 mini-batches; after 13 (minutes) and 37 (seconds);  train_loss = 0.781\n",
      "Epoch 03, upto 03750 mini-batches; after 14 (minutes) and 36 (seconds);  train_loss = 0.817\n",
      "Epoch 03, upto 04000 mini-batches; after 15 (minutes) and 36 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 04250 mini-batches; after 16 (minutes) and 35 (seconds);  train_loss = 0.791\n",
      "Epoch 03, upto 04500 mini-batches; after 17 (minutes) and 32 (seconds);  train_loss = 0.810\n",
      "Epoch 03, upto 04750 mini-batches; after 18 (minutes) and 31 (seconds);  train_loss = 0.799\n",
      "Epoch 03, upto 05000 mini-batches; after 19 (minutes) and 28 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 05250 mini-batches; after 20 (minutes) and 26 (seconds);  train_loss = 0.808\n",
      "Epoch 03, upto 05500 mini-batches; after 21 (minutes) and 24 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 05750 mini-batches; after 22 (minutes) and 23 (seconds);  train_loss = 0.808\n",
      "Epoch 03, upto 06000 mini-batches; after 23 (minutes) and 23 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 06250 mini-batches; after 24 (minutes) and 21 (seconds);  train_loss = 0.821\n",
      "Epoch 03, upto 06500 mini-batches; after 25 (minutes) and 20 (seconds);  train_loss = 0.802\n",
      "Epoch 03, upto 06750 mini-batches; after 26 (minutes) and 18 (seconds);  train_loss = 0.811\n",
      "Epoch 03, upto 07000 mini-batches; after 27 (minutes) and 15 (seconds);  train_loss = 0.820\n",
      "Epoch 03, upto 07250 mini-batches; after 28 (minutes) and 14 (seconds);  train_loss = 0.800\n",
      "Epoch 03, upto 07500 mini-batches; after 29 (minutes) and 11 (seconds);  train_loss = 0.803\n",
      "Epoch 03, upto 07750 mini-batches; after 30 (minutes) and 09 (seconds);  train_loss = 0.802\n",
      "Epoch 03, upto 08000 mini-batches; after 31 (minutes) and 07 (seconds);  train_loss = 0.812\n",
      "Epoch 03, upto 08250 mini-batches; after 32 (minutes) and 06 (seconds);  train_loss = 0.796\n",
      "Epoch 03, upto 08500 mini-batches; after 33 (minutes) and 04 (seconds);  train_loss = 0.807\n",
      "Epoch 03, upto 08750 mini-batches; after 34 (minutes) and 02 (seconds);  train_loss = 0.797\n",
      "Epoch 03, upto 09000 mini-batches; after 34 (minutes) and 59 (seconds);  train_loss = 0.794\n",
      "Epoch 03, upto 09250 mini-batches; after 35 (minutes) and 58 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 09500 mini-batches; after 36 (minutes) and 57 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 09750 mini-batches; after 37 (minutes) and 55 (seconds);  train_loss = 0.814\n",
      "Epoch 03, upto 10000 mini-batches; after 38 (minutes) and 52 (seconds);  train_loss = 0.797\n",
      "Epoch 03, upto 10250 mini-batches; after 39 (minutes) and 50 (seconds);  train_loss = 0.799\n",
      "Epoch 03, upto 10500 mini-batches; after 40 (minutes) and 49 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 10750 mini-batches; after 41 (minutes) and 48 (seconds);  train_loss = 0.794\n",
      "Epoch 03, upto 11000 mini-batches; after 42 (minutes) and 46 (seconds);  train_loss = 0.822\n",
      "Epoch 03, upto 11250 mini-batches; after 43 (minutes) and 44 (seconds);  train_loss = 0.815\n",
      "Epoch 03, upto 11500 mini-batches; after 44 (minutes) and 42 (seconds);  train_loss = 0.818\n",
      "Epoch 03, upto 11750 mini-batches; after 45 (minutes) and 41 (seconds);  train_loss = 0.807\n",
      "Epoch 03, upto 12000 mini-batches; after 46 (minutes) and 40 (seconds);  train_loss = 0.785\n",
      "Epoch 03, upto 12250 mini-batches; after 47 (minutes) and 37 (seconds);  train_loss = 0.817\n",
      "Epoch 03, upto 12500 mini-batches; after 48 (minutes) and 37 (seconds);  train_loss = 0.813\n",
      "Epoch 03, upto 12750 mini-batches; after 49 (minutes) and 34 (seconds);  train_loss = 0.802\n",
      "Epoch 03, upto 13000 mini-batches; after 50 (minutes) and 33 (seconds);  train_loss = 0.794\n",
      "Epoch 03, upto 13250 mini-batches; after 51 (minutes) and 32 (seconds);  train_loss = 0.829\n",
      "Epoch 03, upto 13500 mini-batches; after 52 (minutes) and 30 (seconds);  train_loss = 0.809\n",
      "Epoch 03, upto 13750 mini-batches; after 53 (minutes) and 28 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 14000 mini-batches; after 54 (minutes) and 26 (seconds);  train_loss = 0.789\n",
      "Epoch 03, upto 14250 mini-batches; after 55 (minutes) and 24 (seconds);  train_loss = 0.831\n",
      "Epoch 03, upto 14500 mini-batches; after 56 (minutes) and 22 (seconds);  train_loss = 0.800\n",
      "Epoch 03, upto 14750 mini-batches; after 57 (minutes) and 21 (seconds);  train_loss = 0.824\n",
      "Epoch 03, upto 15000 mini-batches; after 58 (minutes) and 21 (seconds);  train_loss = 0.822\n",
      "Epoch 03, upto 15250 mini-batches; after 59 (minutes) and 18 (seconds);  train_loss = 0.821\n",
      "Epoch 03, upto 15500 mini-batches; after 60 (minutes) and 16 (seconds);  train_loss = 0.810\n",
      "Epoch 03, upto 15750 mini-batches; after 61 (minutes) and 15 (seconds);  train_loss = 0.794\n",
      "Epoch 03, upto 16000 mini-batches; after 62 (minutes) and 13 (seconds);  train_loss = 0.767\n",
      "Epoch 03, upto 16250 mini-batches; after 63 (minutes) and 10 (seconds);  train_loss = 0.822\n",
      "Epoch 03, upto 16500 mini-batches; after 64 (minutes) and 07 (seconds);  train_loss = 0.796\n",
      "Epoch 03, upto 16750 mini-batches; after 65 (minutes) and 04 (seconds);  train_loss = 0.783\n",
      "Epoch 03, upto 17000 mini-batches; after 66 (minutes) and 02 (seconds);  train_loss = 0.826\n",
      "Epoch 03, upto 17250 mini-batches; after 67 (minutes) and 01 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 17500 mini-batches; after 68 (minutes) and 00 (seconds);  train_loss = 0.815\n",
      "Epoch 03, upto 17750 mini-batches; after 68 (minutes) and 58 (seconds);  train_loss = 0.783\n",
      "Epoch 03, upto 18000 mini-batches; after 69 (minutes) and 56 (seconds);  train_loss = 0.826\n",
      "Epoch 03, upto 18250 mini-batches; after 70 (minutes) and 54 (seconds);  train_loss = 0.800\n",
      "Epoch 03, upto 18500 mini-batches; after 71 (minutes) and 53 (seconds);  train_loss = 0.794\n",
      "Epoch 03, upto 18750 mini-batches; after 72 (minutes) and 52 (seconds);  train_loss = 0.827\n",
      "Epoch 03, upto 19000 mini-batches; after 73 (minutes) and 52 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 19250 mini-batches; after 74 (minutes) and 52 (seconds);  train_loss = 0.811\n",
      "Epoch 03, upto 19500 mini-batches; after 75 (minutes) and 51 (seconds);  train_loss = 0.820\n",
      "Epoch 03, upto 19750 mini-batches; after 76 (minutes) and 50 (seconds);  train_loss = 0.800\n",
      "Epoch 03, upto 20000 mini-batches; after 77 (minutes) and 50 (seconds);  train_loss = 0.785\n",
      "Epoch 03, upto 20250 mini-batches; after 78 (minutes) and 50 (seconds);  train_loss = 0.807\n",
      "Epoch 03, upto 20500 mini-batches; after 79 (minutes) and 48 (seconds);  train_loss = 0.763\n",
      "Epoch 03, upto 20750 mini-batches; after 80 (minutes) and 47 (seconds);  train_loss = 0.816\n",
      "Epoch 03, upto 21000 mini-batches; after 81 (minutes) and 48 (seconds);  train_loss = 0.796\n",
      "Epoch 03, upto 21250 mini-batches; after 82 (minutes) and 46 (seconds);  train_loss = 0.808\n",
      "Epoch 03, upto 21500 mini-batches; after 83 (minutes) and 45 (seconds);  train_loss = 0.810\n",
      "Epoch 03, upto 21750 mini-batches; after 84 (minutes) and 44 (seconds);  train_loss = 0.800\n",
      "Epoch 03, upto 22000 mini-batches; after 85 (minutes) and 43 (seconds);  train_loss = 0.783\n",
      "Epoch 03, upto 22250 mini-batches; after 86 (minutes) and 43 (seconds);  train_loss = 0.788\n",
      "Epoch 03, upto 22500 mini-batches; after 87 (minutes) and 43 (seconds);  train_loss = 0.812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03, upto 22750 mini-batches; after 88 (minutes) and 43 (seconds);  train_loss = 0.793\n",
      "Epoch 03, upto 23000 mini-batches; after 89 (minutes) and 42 (seconds);  train_loss = 0.792\n",
      "Epoch 03, upto 23250 mini-batches; after 90 (minutes) and 41 (seconds);  train_loss = 0.783\n",
      "Epoch 03, upto 23500 mini-batches; after 91 (minutes) and 40 (seconds);  train_loss = 0.799\n",
      "Epoch 03, upto 23750 mini-batches; after 92 (minutes) and 40 (seconds);  train_loss = 0.803\n",
      "Epoch 03, upto 24000 mini-batches; after 93 (minutes) and 38 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 24250 mini-batches; after 94 (minutes) and 38 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 24500 mini-batches; after 95 (minutes) and 37 (seconds);  train_loss = 0.788\n",
      "Epoch 03, upto 24750 mini-batches; after 96 (minutes) and 36 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 25000 mini-batches; after 97 (minutes) and 36 (seconds);  train_loss = 0.777\n",
      "Epoch 03, upto 25250 mini-batches; after 98 (minutes) and 36 (seconds);  train_loss = 0.813\n",
      "Epoch 03, upto 25500 mini-batches; after 99 (minutes) and 35 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 25750 mini-batches; after 100 (minutes) and 34 (seconds);  train_loss = 0.837\n",
      "Epoch 03, upto 26000 mini-batches; after 101 (minutes) and 33 (seconds);  train_loss = 0.817\n",
      "Epoch 03, upto 26250 mini-batches; after 102 (minutes) and 33 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 26500 mini-batches; after 103 (minutes) and 32 (seconds);  train_loss = 0.795\n",
      "Epoch 03, upto 26750 mini-batches; after 104 (minutes) and 33 (seconds);  train_loss = 0.810\n",
      "Epoch 03, upto 27000 mini-batches; after 105 (minutes) and 32 (seconds);  train_loss = 0.832\n",
      "Epoch 03, upto 27250 mini-batches; after 106 (minutes) and 31 (seconds);  train_loss = 0.787\n",
      "Epoch 03, upto 27500 mini-batches; after 107 (minutes) and 30 (seconds);  train_loss = 0.812\n",
      "Epoch 03, upto 27750 mini-batches; after 108 (minutes) and 29 (seconds);  train_loss = 0.799\n",
      "Epoch 03, upto 28000 mini-batches; after 109 (minutes) and 27 (seconds);  train_loss = 0.802\n",
      "Epoch 03, upto 28250 mini-batches; after 110 (minutes) and 26 (seconds);  train_loss = 0.806\n",
      "Epoch 03, upto 28500 mini-batches; after 111 (minutes) and 24 (seconds);  train_loss = 0.776\n",
      "Epoch 03, upto 28750 mini-batches; after 112 (minutes) and 23 (seconds);  train_loss = 0.814\n",
      "Epoch 03, upto 29000 mini-batches; after 113 (minutes) and 22 (seconds);  train_loss = 0.795\n",
      "Epoch 03, upto 29250 mini-batches; after 114 (minutes) and 20 (seconds);  train_loss = 0.789\n",
      "Epoch 03, upto 29500 mini-batches; after 115 (minutes) and 20 (seconds);  train_loss = 0.827\n",
      "Epoch 03, upto 29750 mini-batches; after 116 (minutes) and 19 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 30000 mini-batches; after 117 (minutes) and 18 (seconds);  train_loss = 0.781\n",
      "Epoch 03, upto 30250 mini-batches; after 118 (minutes) and 19 (seconds);  train_loss = 0.794\n",
      "Epoch 03, upto 30500 mini-batches; after 119 (minutes) and 16 (seconds);  train_loss = 0.791\n",
      "Epoch 03, upto 30750 mini-batches; after 120 (minutes) and 15 (seconds);  train_loss = 0.801\n",
      "Epoch 03, upto 31000 mini-batches; after 121 (minutes) and 15 (seconds);  train_loss = 0.790\n",
      "Epoch 03, upto 31250 mini-batches; after 122 (minutes) and 15 (seconds);  train_loss = 0.810\n",
      "Epoch 03, upto 31500 mini-batches; after 123 (minutes) and 14 (seconds);  train_loss = 0.790\n",
      "Epoch 03, upto 31750 mini-batches; after 124 (minutes) and 13 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 32000 mini-batches; after 125 (minutes) and 13 (seconds);  train_loss = 0.794\n",
      "Epoch 03, upto 32250 mini-batches; after 126 (minutes) and 12 (seconds);  train_loss = 0.801\n",
      "Epoch 03, upto 32500 mini-batches; after 127 (minutes) and 12 (seconds);  train_loss = 0.783\n",
      "Epoch 03, upto 32750 mini-batches; after 128 (minutes) and 12 (seconds);  train_loss = 0.806\n",
      "Epoch 03, upto 33000 mini-batches; after 129 (minutes) and 10 (seconds);  train_loss = 0.811\n",
      "Epoch 03, upto 33250 mini-batches; after 130 (minutes) and 10 (seconds);  train_loss = 0.806\n",
      "Epoch 03, upto 33500 mini-batches; after 131 (minutes) and 10 (seconds);  train_loss = 0.823\n",
      "Epoch 03, upto 33750 mini-batches; after 132 (minutes) and 10 (seconds);  train_loss = 0.788\n",
      "Epoch 03, upto 34000 mini-batches; after 133 (minutes) and 09 (seconds);  train_loss = 0.787\n",
      "Epoch 03, upto 34250 mini-batches; after 134 (minutes) and 09 (seconds);  train_loss = 0.794\n",
      "Epoch 03, upto 34500 mini-batches; after 135 (minutes) and 09 (seconds);  train_loss = 0.778\n",
      "Epoch 03, upto 34750 mini-batches; after 136 (minutes) and 08 (seconds);  train_loss = 0.803\n",
      "Epoch 03, upto 35000 mini-batches; after 137 (minutes) and 09 (seconds);  train_loss = 0.784\n",
      "Epoch 03, upto 35250 mini-batches; after 138 (minutes) and 09 (seconds);  train_loss = 0.808\n",
      "Epoch 03, upto 35500 mini-batches; after 139 (minutes) and 10 (seconds);  train_loss = 0.790\n",
      "Epoch 03, upto 35750 mini-batches; after 140 (minutes) and 11 (seconds);  train_loss = 0.793\n",
      "Epoch 03, upto 36000 mini-batches; after 141 (minutes) and 10 (seconds);  train_loss = 0.801\n",
      "Epoch 03, upto 36250 mini-batches; after 142 (minutes) and 09 (seconds);  train_loss = 0.802\n",
      "Epoch 03, upto 36500 mini-batches; after 143 (minutes) and 10 (seconds);  train_loss = 0.808\n",
      "Epoch 03, upto 36750 mini-batches; after 144 (minutes) and 10 (seconds);  train_loss = 0.778\n",
      "Epoch 03, upto 37000 mini-batches; after 145 (minutes) and 11 (seconds);  train_loss = 0.780\n",
      "Epoch 03, upto 37250 mini-batches; after 146 (minutes) and 11 (seconds);  train_loss = 0.779\n",
      "Epoch 03, upto 37500 mini-batches; after 147 (minutes) and 11 (seconds);  train_loss = 0.799\n",
      "Epoch 03, upto 37750 mini-batches; after 148 (minutes) and 12 (seconds);  train_loss = 0.839\n",
      "Epoch 03, upto 38000 mini-batches; after 149 (minutes) and 12 (seconds);  train_loss = 0.815\n",
      "Epoch 03, upto 38250 mini-batches; after 150 (minutes) and 12 (seconds);  train_loss = 0.815\n",
      "Epoch 03, upto 38500 mini-batches; after 151 (minutes) and 12 (seconds);  train_loss = 0.792\n",
      "Epoch 03, upto 38750 mini-batches; after 152 (minutes) and 13 (seconds);  train_loss = 0.791\n",
      "Epoch 03, upto 39000 mini-batches; after 153 (minutes) and 12 (seconds);  train_loss = 0.821\n",
      "Epoch 03, upto 39250 mini-batches; after 154 (minutes) and 12 (seconds);  train_loss = 0.825\n",
      "Epoch 03, upto 39500 mini-batches; after 155 (minutes) and 12 (seconds);  train_loss = 0.786\n",
      "Epoch 03, upto 39750 mini-batches; after 156 (minutes) and 12 (seconds);  train_loss = 0.792\n",
      "Epoch 03, upto 40000 mini-batches; after 157 (minutes) and 12 (seconds);  train_loss = 0.802\n",
      "Epoch 03, upto 40250 mini-batches; after 158 (minutes) and 11 (seconds);  train_loss = 0.795\n",
      "Epoch 03, upto 40500 mini-batches; after 159 (minutes) and 12 (seconds);  train_loss = 0.808\n",
      "Epoch 03, upto 40750 mini-batches; after 160 (minutes) and 11 (seconds);  train_loss = 0.788\n",
      "Epoch 03, upto 41000 mini-batches; after 161 (minutes) and 11 (seconds);  train_loss = 0.807\n",
      "Epoch 03, upto 41250 mini-batches; after 162 (minutes) and 12 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 41500 mini-batches; after 163 (minutes) and 13 (seconds);  train_loss = 0.824\n",
      "Epoch 03, upto 41750 mini-batches; after 164 (minutes) and 11 (seconds);  train_loss = 0.784\n",
      "Epoch 03, upto 42000 mini-batches; after 165 (minutes) and 12 (seconds);  train_loss = 0.804\n",
      "Epoch 03, upto 42250 mini-batches; after 166 (minutes) and 11 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 42500 mini-batches; after 167 (minutes) and 10 (seconds);  train_loss = 0.785\n",
      "Epoch 03, upto 42750 mini-batches; after 168 (minutes) and 10 (seconds);  train_loss = 0.809\n",
      "Epoch 03, upto 43000 mini-batches; after 169 (minutes) and 10 (seconds);  train_loss = 0.827\n",
      "Epoch 03, upto 43250 mini-batches; after 170 (minutes) and 10 (seconds);  train_loss = 0.805\n",
      "Epoch 03, upto 43500 mini-batches; after 171 (minutes) and 11 (seconds);  train_loss = 0.803\n",
      "Epoch 03, upto 43750 mini-batches; after 172 (minutes) and 10 (seconds);  train_loss = 0.791\n",
      "Epoch 03, upto 44000 mini-batches; after 173 (minutes) and 10 (seconds);  train_loss = 0.797\n",
      "==================================================================================================\n",
      "Epoch 04, upto 00250 mini-batches; after 01 (minutes) and 01 (seconds);  train_loss = 0.787\n",
      "Epoch 04, upto 00500 mini-batches; after 01 (minutes) and 59 (seconds);  train_loss = 0.805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04, upto 00750 mini-batches; after 02 (minutes) and 59 (seconds);  train_loss = 0.772\n",
      "Epoch 04, upto 01000 mini-batches; after 03 (minutes) and 57 (seconds);  train_loss = 0.803\n",
      "Epoch 04, upto 01250 mini-batches; after 04 (minutes) and 56 (seconds);  train_loss = 0.803\n",
      "Epoch 04, upto 01500 mini-batches; after 05 (minutes) and 55 (seconds);  train_loss = 0.804\n",
      "Epoch 04, upto 01750 mini-batches; after 06 (minutes) and 54 (seconds);  train_loss = 0.790\n",
      "Epoch 04, upto 02000 mini-batches; after 07 (minutes) and 52 (seconds);  train_loss = 0.788\n",
      "Epoch 04, upto 02250 mini-batches; after 08 (minutes) and 51 (seconds);  train_loss = 0.801\n",
      "Epoch 04, upto 02500 mini-batches; after 09 (minutes) and 51 (seconds);  train_loss = 0.818\n",
      "Epoch 04, upto 02750 mini-batches; after 10 (minutes) and 51 (seconds);  train_loss = 0.814\n",
      "Epoch 04, upto 03000 mini-batches; after 11 (minutes) and 51 (seconds);  train_loss = 0.798\n",
      "Epoch 04, upto 03250 mini-batches; after 12 (minutes) and 50 (seconds);  train_loss = 0.786\n",
      "Epoch 04, upto 03500 mini-batches; after 13 (minutes) and 50 (seconds);  train_loss = 0.800\n",
      "Epoch 04, upto 03750 mini-batches; after 14 (minutes) and 48 (seconds);  train_loss = 0.806\n",
      "Epoch 04, upto 04000 mini-batches; after 15 (minutes) and 50 (seconds);  train_loss = 0.777\n",
      "Epoch 04, upto 04250 mini-batches; after 16 (minutes) and 49 (seconds);  train_loss = 0.796\n",
      "Epoch 04, upto 04500 mini-batches; after 17 (minutes) and 50 (seconds);  train_loss = 0.782\n",
      "Epoch 04, upto 04750 mini-batches; after 18 (minutes) and 50 (seconds);  train_loss = 0.801\n",
      "Epoch 04, upto 05000 mini-batches; after 19 (minutes) and 50 (seconds);  train_loss = 0.801\n",
      "Epoch 04, upto 05250 mini-batches; after 20 (minutes) and 51 (seconds);  train_loss = 0.799\n",
      "Epoch 04, upto 05500 mini-batches; after 21 (minutes) and 51 (seconds);  train_loss = 0.808\n",
      "Epoch 04, upto 05750 mini-batches; after 22 (minutes) and 51 (seconds);  train_loss = 0.770\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print('======================================== Training started ========================================')\n",
    "for epoch in range(num_epochs):\n",
    "    print('==================================================================================================')\n",
    "    #model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for i, data in enumerate(dataLoader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device,dtype=torch.float) \n",
    "        labels = labels.type('torch.LongTensor').to(device)\n",
    "        ##with torch.set_grad_enabled(True):\n",
    "        # zero the parameter gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.sum().backward()\n",
    "        optim.step()\n",
    "        # print statistics\n",
    "        \n",
    "        running_loss += loss.mean()\n",
    "        \n",
    "        if i % 250 == 249:    # print every 400 mini-batches\n",
    "            t1 = time.time()\n",
    "            print('Epoch %02d, upto %05d mini-batches; after %02d (minutes) and %02d (seconds);  train_loss = %.3f'%\n",
    "                  (epoch + 1, i + 1, (t1 - t0) // 60, (t1 - t0)%60, running_loss / 250.55))\n",
    "            running_loss = 0.0\n",
    "print('==================================================================================================')\n",
    "print('======================================== Finished Training =======================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  60000 tiles; bacth_size = 6; count on per 400 mini-batches ==> from 11.6 to 12.2 Gb; GPU maximum 49%; CPU maximum 191%\n",
    "##  72000 tiles; bacth_size = 8; count on per 1000 mini-batches => from 11.7 to 13 Gb; GPU maximum 72%; CPU maximum 198%\n",
    "# 200000 tiles; bacth_size = 8; count on per 400 mini-batches => from 12.7 to 13 Gb; GPU maximum 62%; CPU maximum 187% \n",
    "## full tiles; bacth_size = 8; count on per 400 mini-batches => from 12.7 to 13 Gb; GPU maximum 62%; CPU maximum 187% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
